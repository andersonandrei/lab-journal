-# -- mode: org --
# -- coding: utf-8 --
#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:  Andrei's Journal
#+AUTHOR:      Andrei
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) Paper(p) Presentation(p) Scheduler(S) Denis(D) Clément(C) Andrei(a) Qarnot(q) WeekReview(w) CodeReviewed(c)
#+TAGS: DataVis(v) PaperReview(W) MasterThesis(m)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!) | REPORT(r!)

* Week 06-12 / 13-02
** wednesday, 06-02-2019
:LOGBOOK:  
- State "TODO"       from ""           [2019-03-13 qua 15:16]
:END:      
*** 18:02 Meeting, org-journal                                    :OrgMode:
**** Starting on org-journal
***** Trying o do something in the org-jornal1. 
****** Creating a list
     1. Hello
     2. Hi
     3. Salut
	1. Ça vá?
     4. Learning how to do some lists
     5. Creating a lot of topics

**** Todays meeting:
***** Specified each part in the project.
***** Actually, we have two main layers to keep attention: scheduler and allocation.
***** My work will be on the allocation: *DO THE FIRST VERSION OF THE SCHEDULER*:
****** For this, I will use some data from:
******* The INPUT (jobs)
******** Priority
******** Client
******** etc
******* The QBoxes (status from each QBOX):
******** Local data
******** Status(how full it is)

**** DONE LIST
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-13 qua 18:16]
:END:      
***** DONE Learn BatSim
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-07 qui 14:29]
- State "TODO"       from "WAITING"    [2019-02-07 qui 14:29]
- State "WAITING"    from "TODO"       [2019-02-07 qui 14:28] \\
  Waiting ...
****** WAITIGN Read about it
:END:      
*:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-02-07 qui 14:36] \\
  GBOOK:  
- State "DONE"       from "TODO"       [2019-02-11 seg 17:35]
:END:      
:END:
****** DONE First examples
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-11 seg 17:* TODO Install
***** DONE Read two papers
:LOGBOOK:  
- State "DONE"       from "WAITING"    [2019-02-13 qua 18:16]
- State "WAITING"    from "DONE"       [2019-02-11 seg 17:35]
- State "DONE"       from "TODO"       [2019-02-11 seg 17:35]
:END:      
****** DONE How future buildings...
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-13 qua 18:16]
:END:      
****** DONE Heating as a cloud...
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-13 qua 18:16]
:END:      

** thursday, 07-02-2019...
*** Journal

**** Some tips with Pedro

***** Headers

***** Tags

***** etc

*** Research

**** BATSIM

***** DONE First examples on BATSIM website
:LOGBOOK:  
- State "DONE"       from ""           [2019-02-08 sex 18:00]
- State "WAITING"    from "TODO"       [2019-02-07 qui 17:52] \\
  The installation is not working
:END:      

***** DONE First example on GitLab Prototype repo
DEADLINE: <2019-02-08 sex>
:LOGBOOK:  
- State "DONE"       from "WAITING"    [2019-02-11 seg 17:33]
- State "WAITING"    from "TODO"       [2019-02-07 qui 17:52] \\
  The installation of BATSIM is not working
:
** friday, 08-02-2019
*** Batsim
**** I installed it and performed the first example that includes: exectution and statistics.
**** If I understood well the structure. It is:
1. Batsim -> Simulates everything.
2. A scheduler -> Takes the decisions.

***** To see everything running, we can use 2 windows, one for each thing.
*NOTE:* Here, everything was setted to be in the /tmp.

#+NAME: batsim-side
#+BEGIN_SRC <bash> 
  batsim -p /tmp/batsim-v3.0.0/platforms/cluster512.xml        
         -w /tmp/batsim-v3.0.0/workloads/test_batsim_paper_workload_seed1.json
         -e "/tmp/expe-out/out"
#+END_SRC
It will keep the batsim oppened, waiting for the scheduler.

#+NAME: scheduler-side
#+BEGIN_SRC <bash>
  robin generate ./expe.yaml       
                    --output-dir=/tmp/expe-out       
                    --batcmd="batsim -p /tmp/batsim-v3.0.0/platforms/cluster512.xml 
                 -w /tmp/batsim-v3.0.0/workloads/test_batsim_paper_workload_seed1.json 
                 -e /tmp/expe-out/out"       
                    --schedcmd='batsched -v easy_bf'
#+END_SRC 
 It will use robin to run the scheduler batsched with the mode easy_bf.
*** pybatsim
**** Runs a schedular for the batsim.
**** Configuration
***** To install by: pip install pybatsim
***** To clone [[https://gitlab.inria.fr/batsim/pybatsim][PyBatsim-repository]] to have access to the schedulers implemented there.
**** To run its scheduler:
***** To run the batsim as the same way.
***** To run the schedulers, acess the repository and try:
****** pybatsim schedulers/scheduler.py
***** I tried:
****** pybatsim schedulers/fillerSched.py
****** pybatsim schedulers/schedFcfs.py
*** statistics
**** The batsim mainpage offer a example of statistic analysis:
#+BEGIN_LaTeX

#+END_LaTe
#+BEGIN_LaTeX

#+END_LaTeX
 #+NAME: batsim-analysis
 #+BEGIN_SRC sh
 #!/usr/bin/env Rscript
  library('tidyverse') # Use the tidyverse library.
  theme_set(theme_bw()) # Cosmetics.

  jobs = read_csv('out_jobs.csv') # Read the jobs file.

  # Manually compute some metrics on each job.
  jobs = jobs %>% mutate(slowdown = (finish_time - starting_time) /
                                  (finish_time - submission_time),
                       longer_than_one_minute = execution_time > 60)

  # Manually compute aggregated metrics.
  # Here, the mean waiting time/slowdown for jobs with small execution time.
  metrics = jobs %>% filter(longer_than_one_minute == FALSE) %>%
    summarize(mean_waiting_time = mean(waiting_time),
              mean_slowdown = mean(slowdown))

  print(metrics) # Print aggregated metrics.

  # Visualize what you want...
  # Is there a link between jobs' waiting time and size?
  ggplot(jobs) +
    geom_point(aes(y=waiting_time, x=requested_number_of_resources)) +
    ggsave('plot_wt_size.pdf')

  # Is this still true depending on job execution time?
  ggplot(jobs) +
    geom_point(aes(y=waiting_time, x=requested_number_of_resources)) +
    facet_wrap(~longer_than_one_minute) +
    ggsave('plot_wt_size_exectime.pdf')

  # Is there a link with job size and execution time?
  ggplot(jobs) +
    geom_violin(aes(factor(requested_number_of_resources), execution_time)) +
    ggsave('plot_exectime_size.pdf')

 #+END_SRC
**** Running this analysis on both pybatsimexamples we can check the different results.

** monday, 11-02-2019

*** DONE To understand:
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-11 seg 17:07]
:END:      

**** DONE The INPUT format for batsim;
:LOGBOOK:  
- State "DONE"       from "CANCELLED"  [2019-02-11 seg 17:07]
:END:      

**** DONE Some schedular examples;
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-11 seg 17:07]
:END:

*** The workflow:

**** Batsim-Scheduler communication:

***** Messages, JSON, via request-reply model:

****** Contraints format:

******* now

******* events

******** timestamp

******** type

******** data

******* And to differ the message, we change the *event type* as :

******** BIDIRECTIONAL

******** BATSIM => SCHEDULER

******** SCHEDULER => BATSIM

***** Workload:

****** The workload is an Input combined as:

******* Jobs: Users requests. It has:

******** id, subtime, res, profile, walltime, +
******* Profiles: Defines how the job execution should be simulated. It has:
******** type, etc. Where the type could be:
********* delay, prallel task, homogeneous pararllel task, etc.

**** Batsim requires to start:

***** a plataform; a workload; an output folder.

****** Providing a worload, it will have the jobs that should be scheduled.

***** Then, batsim will be learning, waiting for a scheduler to manage the jobs.

**** The scheduler:

***** Once the Batsim is already runnig, when we run the scheduler it will communicate with the Batsim by the messages, doing the requested decision.

***** The schdulers should implement all possible actions asked by the message types. For example: JobInitialize,kill,resquest. onBatSimInit,onJobSubmission, onJobCompletion.
**** An example:

***** I understood the fillerSched.py scheduler. It works following:

1. Initialize everything after Batsim intialized.
2. Schedule the jobs.
 2.2 _OnAfterBatsimInit_: // _Read_ a list of jobs *OpenJob* and a list of resources *availableResources* 
 2.1 _scheduleJobs_: // _Check_ all jobs in *OpenJob*
   2.1.1 _if_ (job.resourcesRequested > *aivailableResources*)
            discard it and remove from the *OpenJob*
   2.1.2 _else_
            scheduleJobs.append(job)
            *availableResources* -= jog.resourceRequested
            updateConsumptionTime
 2.2 _OnJobSubmission_:
  2.2.1 openJob.add(job)
  2.2.2 scheduleJobs()
 2.3 _OnJobCompletion_:
  2.3.1 *availableResources* += job.resourceRequested
  2.3.2 scheduleJobs()
***** I ran it as:
batsim -p platform52.xml -w test_batsim_paper_workload_seed1.json -e test-out-2
launcher.py scheduler/fillerSched.py

** tuesday, 12-02-2019
*** DONE on Batsim
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-13 qua 18:18]
:END:      
**** DONE Check about the data asked for the jobs. How to locate or transfer it.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-13 qua 08:57]
:END:
***** It is done by writing and checking the NFS file after and before to write or to remove some data from some QBox.
*** DONE on Papers
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-13 qua 18:18]
:END:      

**** DONE Check on the Qarnot gitlab if there are some techniques for the schedulers.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-13 qua 18:17]
:END:

***** There is a Deliverable2.2a that show the algortith to be implemented.
**** DONE Search some papers for schedulers on Cloud Computing
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-13 qua 18:17]
:END:      
*** TODO Source block configuration orgmode
:LOGBOOK:  
- State "TODO"       from ""           [2019-02-12 ter 12:54]
:END:
*** TODO ESS library
:LOGBOOK:  
- State "TODO"       from ""           [2019-02-12 ter 12:56]
:END:
** wednesday, 13-02-2019
*** Qarnot meeting
**** Administrative and update things with other teams.
**** About my part I should finish the current version of the QNodes scheduler. This way we will have a full system working.
**** With a full system working we will submit a paper to *SC2019*.
*** Papers and techniques
**** I read the both papers that I selected on 06-02 and 12-02 and selected some algorthms to check later.
**** But, as our plan now is to finish the current scheduler version, I will work on the current code and think about improvements after (aka. read about techniques now).
** REPORT I worked, mainly, understanding the problem and the behvaior. :WeekReview:
* Week 14-02 / 20-02
** thursday, 14-02-2019
*** DONE Modify the schedulers on pybatsim and compare the differences.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-15 sex 10:47]
:END:      
 I did it on the fillerSched. Now I will start to try to write a pseudocode for the QNodes scheduler.
** friday, 15-02-2019
*** I should implement the algorithm of the Deliverable 2.2a. But, it asks for a function to predict the time to download a dataset for a specific QBox.
*** I asked to Alex, by Slack, and he answered me that they do not have idea how to implement it now. So, I should skip it now, and after choose another rule to use.
*** DONE Start to write a pseudocode to the algorithm on dlv.2.2a. 
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-15 sex 21:21]
:END:      
**** def schedule(self, job): 
        print("Haaaaaaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeey-------\n")
        
        print("Job: ", job.id)
        print("Subtime: ", job.submit_time)
        print("Job.profile", job.profile)
        print("Profile", type(self.bs.profiles))
        list_of_datasets = {}
        for key in self.bs.profiles:
            print(self.bs.profiles[key][job.profile]['datasets'])
            qbox_key = job.profile
            list_of_datasets[qbox_key] = self.bs.profiles[key][job.profile]['datasets']

        for s in self.storage_controller._storages:
            st = self.storage_controller.get_storage(s)
            print("Datasets on Qbox: ", st.get_datas
**** It is current : 
***** getting the datasets asked by a job
***** listing all the storages on the StorageControl and its datasets.
*** 
*** TODO Talk with Clement
**** nix-shell https://github.com/oar-team/kapack/archive/master.tar.gz -A pybatsim
**** Cant found batsim using it.
**** 
**** On the batsim command: --events ../events/greco/events.json . There is no events.json on the folder.
**** 
**** Should I populate the Storage on the QNodeSched?
** monday, 18-02-2019
*** I finished my first version of the list of QBoxes that already has the specified dataset.
*** DONE 
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-19 ter 10:46]
:END:      
**** DONE Ask Clément
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-19 ter 10:45]
:END:      
***** Should I consider only one dataset per job and profile?
****** If more than one dataset per job: Should I do a matrix of QBoxes that has each dataset and chose the one that has more datasets?
****** No.
***** Should I consider a dataset as ["ds1"] or at the presented way ["QB...:inpu, QB:...:0, QB...:..."] ?
***** Can I commit and push my modifications in my branch on github?
****** Yes.
***** Mainly, I should put in the List only the QBoxes that already have ALL the required datasets from a job.
** tuesday, 19-02-2019
*** TODO
**** Check to put the list_qboxes_with_datasets() on the StorageController.
**** Dispatche some jobs to some QBoxes to test.
** wednesday, 20-02-2019
*** I attended the presentation:
**** David Shmoys: Models and algorithms for the Operation and Design of Bike-Sharing System
*** I finished my report to HPC course.
** REPORT I kept working to understand the environment and to learn how to use the Batsim and PyBatsim. :WeekReview:
* Week 21-02 / 27-02
** thursday, 21-02-2019
*** Checked one more time the function to do the list L (the list of qboxes that already has the required datasets)
*** Try to submitt the jobs to the QBoxes.
**** Here, the QNode uses onSubmission(job) to send it to the QBox.
**** It is receiving the message JOB_REJECTED. Maybe I need to use the "events" to change the event type of the jobs.
*** Algorithm
**** Im thinking in:
***** for each job j:
****** sched = True
****** l = L(j)
****** if l != null:
******* qbox = maxHeatingReq(l):
******* if qbox == null:
******** qbox = l [ 0 ]
****** else :
******* qbox = maxHeating()
******* if qbox == null:
******** sched = False
****** if sched:
******* qbox =  max_requiringHeating()
****** else:
******* waitingList.append(j)
***** 
** friday, 22-02-2019
*** TODO Verify how to:
**** DONE How to run a job, and why mine are been rejected?      :Clément:
:LOGBOOK:  
- State "DONE"       from "WAITING"    [2019-02-25 seg 11:05]
- State "WAITING"    from "TODO"       [2019-02-25 seg 11:05] \\
  Ignore it now. I will start to work in the last version of the code.
:END:      
**** DONE Verify how to manage the instances of a job            :Clément:
:LOGBOOK:  
- State "DONE"       from "WAITING"    [2019-02-25 seg 11:05]
- State "WAITING"    from "WAITING"    [2019-02-23 sáb 04:08] \\
  I need to confirm, but, as I understood, in the workload we have jobs like: 
    job 0 = {id=codeX-0 ...} job 1 = {id=codeX-1 ...} ... Job n = {id = codeX-n} }
  So, each job with id started with "codeX" , for real, are tasks for the same job.
  So, a job could be a unique job, or, if are composed by others, its not a job, it is a task.
  Then, we read the jobs as QTask().

  *YES*

- State "WAITING"    from "TODO"       [2019-02-22 sex 15:06] \\
  Is each instance a QTask on the new qarnotQNodeSched?
:END:
**** DONE Why the QTask now? What did change?                    :Clément:
:LOGBOOK:  
- State "DONE"       from "WAITING"    [2019-02-25 seg 11:05]
- State "WAITING"    from "WAITING"    [2019-02-23 sáb 04:12] \\
  As I commit above, I think that these Taks are jobs that composes other bigger jobs.

  *YES*
:END:      
**** DONE Verify which qbox had preemption
:LOGBOOK:  
- State "DONE"       from "WAITING"    [2019-02-25 seg 11:05]
- State "WAITING"    from "TODO"       [2019-02-23 sáb 04:14] \\
  I need to confirm. But as I understood. There are nothing registrating if some QBox has preemption.
  By definition, preemption occurs when some executing job is stopped because another one with more priority arrives.
  I think I need to check, by the priority of each job, if I would put the current job in some QBoxes, 
  it would cause a preemption. So, I need to check the priority of the jobs that are already runnig in that QBox.

  *YES*
:END:
**** WAITING Verify the qbox that require more work for the next hour
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-02-22 sex 10:49] \\
  I did it, but not for the next hour exactly. I do not know how to check it.
  Also, I need to test it, but I do not know how to add heating requirement to a qbox.

  *??* CHECK IT
:END:
** saturday, 23-02-2019
*** I think I understood some previous questions. They are in the TODO list of last day. So, I noted there what I think tha I understood.
** sunday, 24-02-2019
*** I changeg the workload in use, putting the "real data" from the qarnot-examples.
*** I started to do a function to get all indexes of jobs that compose the same main job.
**** This way, each job is thinked as a task, then, the idea is to dispatch as many as possible taks to the same qbox.
** monday, 25-02-2019
*** I will start to work with the last version of the scheduler in the pybatsim-temperature branch.
*** It uses QTask as the read input from the workload (aka. each input is a task and many tasks compose a job).
*** This new one are implement almost the scheduler of the delivrable 2.2a. Like:
*** TODO Algorithm peaces:
**** WAITING L <- List of QBOX that already has the required dataset. :Andrei:
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-02-25 seg 11:19] \\
  It is done in the previous scheduler. I should put it in the new one.
:END:      
**** WAITING Dispatch as many instances of j as possible on the selected QBox. :Clément:
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-02-25 seg 11:20] \\
  Almost done. I should check.
:END:      
**** WAITING Check the priority to QBoxes that have available QRads without preemption. :Clément:
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-02-25 seg 11:20] \\
  It is almost done. I should check.
:END:      
**** TODO Check the QBoxes that requires the most work in the next hour. :Qarnot:
**** TODO Download time prediction of the datasets                :Qarnot:
**** TODO LQ <- List of QBoxes sorted as 1. and 2.
***** WAITING 1. The count of available QRads for the priority of j in descending order. :Clément:
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-02-25 seg 11:20] \\
  It is almost done. I should check it.
:END:      
***** TODO 2. The predicted downloads time of the datasets.      :Qarnot:
*** Andrei's algortihm:

'''
The idea is to order a waiting list of tasks by the profile.
Then, find a list of qboxes that already has the required data set. 
Then, for each qbox in this list, dispacth as many tasks, of the same profile, as possible.
  If there are not enough qboxes to dispatch this tasks, find another options, with other rule.
Update the list and start for the next group (ordered by profiles) of tasks.
'''

 waiting_lits
 ordered_l = waiting_lists.orderedByProfile()
 nb = len(ordered_l)
 while (nb > 0):

   #The amount of tasks with the same profiles, counted by the beginning of the list, until the first task with a different profile.
   nb_same_profile = get_nb_same_profile(ordered_l) 
   
   qboxes_for_profile = L(ordered_l[ 0 ])

   for qb in qboxes_for_profile:
     nqb = qb.resources
     if (nqb >= nb_same_profile):
       dispatch(ordered_l[0:nb_same_profile], qb)
     else:
       dispatch(ordered_l[0:nqb], qb)
       nb_same_profile -= nqb
       ordered_l = ordered_l[nqb:]
   
   if(nb_same_profile > 0):
     findQBoxAndDispatch()
     ordered_l = ordered_l[nb_same_profile:]
*** New command                                                   :OrgMode:
**** To find and replace text: M + %
** tuesday, 26-02-2019
*** DONE Implement the previous algorithm
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-26 ter 18:42]
- The structure are done. But BATSIM is not working with this version of scheduler. 
  So, I need to wait the fixes to check.
:END:
** wednesday, 27-02-2019
*** TODO Work on
**** DONE the priority of QBoxes that have available QRads without preemption
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-01 sex 15:43]
:END:      
***** (aka. Run the taks using the priority levels: background, low and high.
**** CANCELLED the priority of QBoxes that requires the most work in the next hour
:LOGBOOK:  
- State "CANCELLED"  from "TODO"       [2019-02-27 qua 11:10]
:END:      
***** (skip it now)
**** DONE the count of available QRads for the priority of j in descending order
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-01 sex 15:44]
:END:      
***** (aka. The same as the first TODO)
**** WAITING the predicted download time of the data sets.
:LOGBOOK:  
- State "WAITING"    from "CANCELLED"  [2019-02-27 qua 11:10] \\
  Waiting the other team develop it.
- State "CANCELLED"  from "TODO"       [2019-02-27 qua 11:10]
:END:
**** DONE To merge my modifications on the StorageController with the Clément temperature branch.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-02-27 qua 17:17]
:END:      
**** DONE Organize the code. Use the doDispatch() as the main peace of the method schedule(). At the moment, it is duplicated.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-01 sex 15:44]
- [01-03-2019] As I found the error, I rewrote the code.
:END:
** REPORT I did the first steps on my implementation             :WeekReview:
* Week 28-02 / 06-03
** friday, 01-03-2019
*** I found an error in my last algorithm.
*** Let's rewrite it. Done.
*** Clément fixed batsim and changed something in the QNodeSched. I merged the codes.
*** TODO Check on next monday, how to run this version ?
** saturday, 02-03-2019
*** I added the structure to use the predicted download time of the data sets, when it is ready.
** monday, 04-03-2019
*** DONE 
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-04 seg 18:23]
:END:      
**** DONE Run the updated batsim and qarnotNodeSched.py
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-04 seg 18:22]
:END:      
***** To run it, I copied the folder sample-data/simple from simulator-prototype to batsim to make it easier.
***** The command is: ./batsim -p ../sample-data/simple/platform_simple.xml -T 1 --enable-dynamic-jobs --events ../sample-data/simple/events_simple.json -w ../sample-data/simple/workload_simple.json
**** DONE Try to run mine and correct the possible *errors*
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-04 seg 18:22]
:END:      
***** DONE To decide if I will create a QTask using the job.profile, or if I will get it after using the task.job_id :Clément:
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-04 seg 15:47]
- I choose to add into the QTask() the profile. I should merge it with the main qarnotBoxSched.py
:END:
*** I got a little error in the qarnotNodeSched.py. It is getting the number of slot of bkgd, low or hight priority for each qmobo :Clément:
*** but, it is not verifying if the value is zero. So, the execution have never passed from the first priority case, bkgd.
**** Clément already fixed it in his official version.
*** I ran my first version of the scheduler !!!!!!!!!!!!!!  
**** It is working.
**** I generated the csv files to it and to the qarnotNodeSched to try to compare some values:
***** The allocated_resources in the _jobs.csv changed. Mine has in almost the cases, less allocation.
***** If I understood well, it shows how many resources were allocated for each job. So, it show that mine is allocating less resources,
***** I am concluding it means that the allocation decision is working, and the tasks or jobs with the same profile (the tasks that composes the same job) are not allocating new resources everytime.
**** Now, the next step is to check some more workloads and really compare the performance with the official one.
** tuesday, 05-03-2019
*** TODO To Compare the results
**** DONE First point: [Evalys][[https://gitlab.inria.fr/batsim/evalys]]
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-05 ter 18:28]
- It shows some parameter for the jobs and workload. Using it right now, I am looking to the Gantt Chart to see how the tasks are beeing scheduled.
:END:
**** TODO To compare the temparatures at the beggining and end of the simulation.
*** Presentation: *Parallel Scheduling of DAGs under Memory Constraints* :Presentation:
**** Use DAGs to describe
**** Problem: Shared memory (Limitade resources)
*** TODO To check the code
**** I am build the list L before checking all states (bkgd, low and high). It is not wrong for results, but maybe I could do only one time and get a better performance.
*** Results                                                           :ATTACH: :ATTACH:

:PROPERTIES:
:Attachments: qarnotNodeSchedAndrei_jobs_results_first.png qarnotNodeSchedAndrei_jobs_results_first_second.png qarnotNodeSchedAndrei_jobs_results_triplet.png qarnotNodeSched_jobs_results.png qarnotNodeSchedAndrei_jobs_results_first13.png qarnotNodeSchedAndrei_jobs_results_first_triplet.png qarnotNodeSchedAndrei_big_jobs_results_first_triplet.png qarnotNodeSchedAndrei_big_jobs_results_first.png qarnotNodeSched_big_jobs_results.png
:ID:       88869df9-d5d4-400e-b11e-b8e41a439426
:END:
 
To compare the difference between the qarnotNodeSched and my implementation I am running the same workload for both and then, I am using the evalys to plot some results and compare it.
For the first moment, I ran the simple wokload, with 8 jobs, and 2 QBoxes which has 7 and 3 QMobos each one.
I got the following results:

First of all, the current scheduler does not use anything about location, neither download or transfer data sets to the QBox where the tasks are dispatched.
It just simulate the job's execution but does not do nothing about the data managment.
So, to check my scheduler, I am adding at the beggining of the simulation some data sets in some QBoxes.
My implementation check if exists a list (L) of QBoxes that already have the required data sets. Then, dispatch the tasks considering this list (L) and the status of the Mobos into the QBoxes (bkgd, low and high).
If L is empty it just consider the status of the Mobos, as the current scheduler.

Here we have the plots for the current qarnotNodeSched:
#+NAME: fig:0
#+ATTR_ORG: :width 700px
[[file:data/88/869df9-d5d4-400e-b11e-b8e41a439426/qarnotNodeSched_jobs_results.png]]

Note that we have tasks from the same job scheduled to different QBoxes.

Now, for all plots, I am adding data sets in some QBoxes, which will change plot by plot.
I will use q1 to the qbox above and q2 to the qbox under:

_______________________________________________________________


DATASET_ADDED: first on q2

#+NAME: fig:1
#+attr_org: :width 500px
[[file:data/88/869df9-d5d4-400e-b11e-b8e41a439426/qarnotNodeSchedAndrei_jobs_results_first.png]]

We can see that all first tasks came down.
______________________________________________________________

DATASET_ADDED: first and second on q2

#+NAME: fig:2
#+attr_org: :width 500px
[[file:data/88/869df9-d5d4-400e-b11e-b8e41a439426/qarnotNodeSchedAndrei_jobs_results_first_second.png]]

We can see that all first tasks came down, as the second task.
_____________________________________________________________

DATASET_ADDED: triple on q2

#+NAME: fig:3
#+attr_orf: :width 500px
[[file:data/88/869df9-d5d4-400e-b11e-b8e41a439426/qarnotNodeSchedAndrei_jobs_results_triplet.png]]

We can see that all triplet tasks came down.

DATASET_ADDED: first on q1

#+NAME: fig:4
#+ATTR_ORG: :width 500px
[[file:data/88/869df9-d5d4-400e-b11e-b8e41a439426/qarnotNodeSchedAndrei_jobs_results_first13.png]]

We can see that all first tasks goes up.

______________________________________________________________

DATASET_ADDED: first and triplet on q2

#+NAME: fig:5
#+ATTR_ORG: :width 500px
[[file:data/88/869df9-d5d4-400e-b11e-b8e41a439426/qarnotNodeSchedAndrei_jobs_results_first_triplet.png]]

We can see that all triplet taks came down, as the first tasks.
_______________________________________________________________

*Looking to the dataset from the Qarnot extractor: 1-day*:

Simulating with the current scheduler:

#+NAME: fig:6
#+ATTR_ORG: :width 500
[[file:data/88/869df9-d5d4-400e-b11e-b8e41a439426/qarnotNodeSched_big_jobs_results.png]]

________________________________________________________________

Now, simulating with my scheduler, adding some datasets we can se the follow plots:

DATASET_ADDED: one randomly 

#+NAME: fig:7
#+ATTR_ORG: :width 500
[[file:data/88/869df9-d5d4-400e-b11e-b8e41a439426/qarnotNodeSchedAndrei_big_jobs_results_first.png]]

_________________________________________________________________

DATASET_ADDED: two randomly

#+NAME: fig:8
#+ATTR_ORG: :width 500
[[file:data/88/869df9-d5d4-400e-b11e-b8e41a439426/qarnotNodeSchedAndrei_big_jobs_results_first_triplet.png]]


For both we can see modifications in the GANTT Chart. As it has a lot of information, it is not so clear to read. 
But we can see the behavior changing in order of the data sets modifications.
_________________________________________________________________

So, I am concluding that my implementation are working in the sens of take care about the location of the data sets that already exists on the QBoxes. :)

** wednesday, 06-02-2019
*** Meeting about the project
**** Continue working on my implementation
**** Pierre's feedback: Try to draw an overview of the actions
***** Like this diagrams:                                        :ATTACH:
:PROPERTIES:
:Attachments: diagram%20model1.jpeg diagram%20model2.jpeg
:ID:       336b7389-3c9a-47f4-af8e-473d7e0891a6
:END:

#+ATTR_ORG :width 200
[[file:data/33/6b7389-3c9a-47f4-af8e-473d7e0891a6/diagram model1.jpeg]]

#+ATTR_ORG :width 200
[[file:data/33/6b7389-3c9a-47f4-af8e-473d7e0891a6/diagram model2.jpeg]]

*** TODO 
**** WAITING To talk with Alex to get the current state of the StorageController
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-03-29 sex 11:39]
:END:      
**** DONE To talk with Pierre to check the worflow and how to analyze the temperature
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-06 qua 16:21]
- We will aks the other team to extract this data from the DB. And so, I can check and plot it.
  It will be a plot like: 3 lines, 1 from the real Qarnot data, 1 from each scheduler. So, it will possible to compare which one is better at this sense.
:END:      
**** DONE To generate different kinds of workloads (more taks than resources) to test it.
:LOGBOOK:  
- State "DONE"       from "WAITING"    [2019-03-29 sex 11:39]
- State "WAITING"    from "TODO"       [2019-03-06 qua 16:23] \\
  The generator uses the real Qarnot Serves/DB, it is so busy now.
:END:

** REPORT I finished the my first version, calculated some results and presented it. :WeekReview:
* Week 07-03 / 13-03
** thursday, 07-03-2019
*** TODO
**** DONE To make the qarnotScheduler works with 1week workload  :Clément:
:LOGBOOK:  
- State "DONE"       from "WAITING"    [2019-03-29 sex 11:31]
- State "WAITING"    from "TODO"       [2019-03-07 qui 17:34] \\
  Almost done. 
  * We added one more line in the QBoxSched, it was a "gambiarra". We should implement the 're-schedule' there.
:END:      
**** DONE To get one day more of simulation data using the qarnot-extractor
:LOGBOOK:  
- State "DONE"       from "WAITING"    [2019-03-29 sex 11:31]
- State "WAITING"    from "TODO"       [2019-03-07 qui 17:31] \\
  Still waiting for a good moment.
:END:      
**** DONE To generate a bigger workload 
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-07 qui 17:31]
- I took the 1 day workload and removed many Qboxes. Now it has more jobs than resources.
:END:      
***** TODO To put two days together
***** TODO TO try to do something like this with the simple workload
*** I also changed the simple workloads. Now it has 4 Mobos and so, jobs are been rejected.
*** It still allocate the jobs as the data set location, but, if there ia a conflict in the submission time with another task,
*** it is rejected.
*** DONE To check with Clément this rejections. I think the jobs should be reeschedule, but we do not have it yet, I think.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-08 sex 10:47]
It happened because the 'gambiarra' that we did to try to run the 1-week workload.
:END:      
** friday, 08-03-2019
*** TODO
**** TODO Implement the re-schedule method.
***** The QBoxes should 'reject' and so, the QNode should put it again in the waiting_list.
***** It is already implemented, but it is confuse because it adds and removes from differente instances of the waiting_list and is getting error because are deleting more than adding.
****** So, I will try to create a reject_list to manage during some bigger step, then put it back on the wainting list.
** sunday, 10-03-2019
*** To continue testing the case of workloads failing.
*** DONE 
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-10 dom 20:49]
:END:      
**** DEFERRED Create a new workload, smaller, but also with more taks than resources.
:LOGBOOK:  
- State "DEFERRED"   from "DONE"       [2019-03-10 dom 20:49]
- State "DONE"       from "TODO"       [2019-03-10 dom 20:49]
:END:      
**** DONE Organize the prints to output
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-10 dom 20:49]
:END:      
*** To try to do some automatic tests for different workloads
*** TODO Correct: Possible Bug's list
**** TODO The taks are not removed from the queue_task after dispatched. Only onJobCompletion(). This way, every doDispatch() try to re-dispatch all taks. 
***** Which does not happen because the number of instances for them is 0. But, anyway, doDispatch() consume times trying to do it.
**** DONE When a job is rejected, the number_instances_left is not increased.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-10 dom 21:21]
:END:      
**** DONE When a job is rejected, the num of available_mobos in tup[] is not increased.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-10 dom 21:21]
:END:       
**** TODO On the QBoxSched:
***** On onDispatchedInstances() :
****** The method scheduleInstace() are receiving the waiting_instances duplicated. So, it is happening in the onDispatchedInstances() at some point.
***** It prints "Still have x instances" already with the duplicated list. 
***** It is possible to be verified printing the waiting_list between the logs "received x intances" and "still have x instances".
**** TODO To verify the mapToQbox() and then implement a removeFromQBox().
** monday, 11-03-2019
*** Some Corrections
**** There was an error on JobCompletion(). It was direct dispatching a job, but was not adding it in the qtask.running_instances.
***** Solved.
**** REPORT There was a Bug on the updateAndReport() on the QBox level. It was getting wrong the available resources, then inform wrong values to the doDispatch() on the QNode level.
:LOGBOOK:  
- State "REPORT"     from ""           [2019-03-29 sex 11:35]
:END:      
***** Because it, NodeSched was dispatching more tasks than possible, making the QBox creates a waiting_instances list.      
***** It was corrected and now the doDispatch are sending valid amount of tasks to each QBox. 
**** In the simple_more workload, the triplet_ tasks are not ending. It is dispatched but have never end. Then, Batsim becomes in deadlock.
***** I changed the priority value of triplet_ from -5 to 15 and it works well for this case. 
**** For 1 week it still not working.
**** DONE To check the priority conditions.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-12 ter 16:36]
- The problem was the direct_dispatch().
- In the availableResouces check().
:END:      
** tuesday, 12-03-2019
*** Experiment table design

| *Workload*  | *Workload* |  *Workload* | *Status*          | *Status*                | *Status*           |
| *Name*      |    *Tasks* | *Resources* | *qarnotNodeSched* | *qarnotNodeSchedAndrei* | *Final allocation* |
|             |            |             |                   |                         |                    |
| simple      |          8 |          10 | PASS              | PASS                    | MODIFIED           |
| simple_more |          8 |           4 | PASS              | PASS                    | MODIFIED           |
| 1day        |        418 |        1000 | PASS              | PASS                    | MODIFIED           |
| 1day_more   |        418 |         724 | PASS              | PASS                    | MODIFIED           |
| 1week       |       3010 |         991 | PASS              | PASS                    | MODIFIED           |
| 1week_more  |       3010 |           x | PASS              | WAITING                 |                    |
|             |            |             |                   |                         |                    |
*** The simulation ran correclty wihtou the direct_dispatch(). So we will will keep like this for the momento and try to re-implement it.
** wednesday, 13-03-2019
*** Qartnot meeting
**** WAITING Checks 
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-03-29 sex 11:36]
:END:      
***** DONE To check with Alex the status of the StorageController
:LOGBOOK:  
- State "DONE"       from "WAITING"    [2019-03-13 qua 10:25]
- State "WAITING"    from "TODO"       [2019-03-13 qua 10:22] \\
  They have something done, but did not push yet.
:END:
**** SC is too near, so we will drop :(
**** The goal is to finish a full implementation until April 16,17 and then, to build a full paper describing the platform.
**** ClustComput could be a conference to apply. https://clustercomp.org/2019/technical/

*** TODO Next steps
**** TODO Comparison for job locations
***** TODO Different workloads
***** TODO Set automatically when Alex finished the StorageController
**** TODO Comparison for the temperature
***** TODO Implements first versions
**** TODO Presentation of the scheduler
***** TODO Draw the workflow
**** TODO Organize my workflow with batsim aka. run_scripts and results
*** Im looking conferences
**** http://www.lanoms.org/2019/#topics
***** Smart Devices and Home Networks
***** Smart Cities, Smart Grids
***** 
**** http://sbqs.sbc.org.br/index.php/pt/chamada-de-trabalho
**** https://webmedia.org.br/2019/
**** http://www.inf.ufrgs.br/er2019/
*** DONE Master2 -> Register my presentation in June with an external expert.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-13 qua 14:14]
:END:      
**** Title: Job allocation in a distribued private cloud
**** External Expert: Christophe Cérin
** REPORT I worked testing both implementation with different workloads. This way I found problem in the main implementation. :WeekReview:
** REPORT It was not working with workloads composed by more jobs than resources. :WeekReview:
** REPORT The problem was the direct_dispatch(), then I commented it for the moment, and I checked both implementation with the 1day and 1week workloads. :WeekReview:
** REPORT Both ran and the location-based implementation is working as well. :WeekReview:
* Week 14-03 / 20/03
** thuersday, 14-03-2019
*** I have started to draw the scheduler workflow
** friday, 15-03-2019
*** I discussed with Clément some components and versions of the diagram and then finish a first version.
*** I made the detailed description of all steps.
*** I showed and discussed improvements with Pierre.
** saturday, 16-03-2019
*** I drew a new version of the diagram, regarding the real components instead of the simulation.
** monday, 18-03-2019
*** WAITING Discuss and repair the new draw
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-03-19 ter 10:27] \\
  I showed it to Denis and did few repairs.
:END:      
*** WAITING Check updates on the StorageController implementation, if any.
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-03-19 ter 10:27] \\
  Clément will check the merge requested form Alex.
:END:
** tuesday, 19-03-2019
*** DONE Check the repairs in the NodeSched, on directDispacth(), and run all workloads.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-19 ter 16:32]
:END:
*** WAITING Use the .csv about the temperature on the platform-extractor to measure the difference of the final temperature between NodeSched e NodeSchedAndrei.
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-03-19 ter 16:32] \\
  I draw two plots and have been started to script it.
:END:      
**** Im thinking to do 2 kinds of plots:
***** 1. For a selected QRad, plot the time on axe x and the temperature on axe y for the three data source (real, sched, schedAndrei)
***** 2. For all QRads, plot the BoxPlots (for whole data source) on axe x, and the temperature on axe y.
**** BUT, there are 228 jobs for 1022 QRads, how to plot for all? How to select which one to plot?
*** I also showed the new version of the diagram (focused on the real platform and componenets).
**** He adviced me to keep both, onde for the simulation and onde for the real platform).
**** He advided me to separate more the component and the lines and explicit more somethings.
** wednesday, 20-03-2019
*** DONE The script to analyze the _temperatures.csv with R.
:LOGBOOK:  
- State "DONE"       from "WAITING"    [2019-04-02 ter 14:42]
- State "WAITING"    from "TODO"       [2019-03-20 qua 16:14] \\
  In progress..
:END:      
**** Installed: r-base, r-studio, ess
**** This link shows something like I want to do : https://stackoverflow.com/questions/14604439/plot-multiple-boxplot-in-one-graph
***** It uses the function 'melt'. Here I found some details https://www.statmethods.net/management/reshape.html
*** DONE Finalize a first version of diagrams.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-20 qua 16:14]
I showed to Pierre and modified some more details. Shower do Clément and Denis also. So, the first version is done.
:END:      
** REPORT I have been waiting for the correction in the qarnotNodeSched and for the StorageController. :WeekReview:
** REPORT While it is not done I have been worked on the diagram. :WeekReview:
** REPORT Clément repaired the qarnotNodeSched then I merge it with my version. :WeekReview:
** REPORT Now we have the _temperature.csv as output of the simulation, then I also have been started to script the plot of the graphs to view the temperature behavior. :WeekReview:
** REPORT I finished the first version of the diagrams (real platform and simulations) :WeekReview:
* Week 21-03 / 27-03
** thuersday, 21-03-2019
*** DONE Check the rpy2
:LOGBOOK:  
- State "DONE"       from "WAITING"    [2019-03-28 qui 13:16]
- State "WAITING"    from "TODO"       [2019-03-21 qui 16:20] \\
  The installation failed.
:END:      
*** I started to use the Jupyter Notebook with R to plot the results.
**** The problem is that the two .csv are not of the same dimension and the rads are not ordered at the same way.
**** So, I'm trying to get the same times and order at the same way.
** friday, 22-03-2019
*** Pedro helped me a lot, he taught me many things in R and many ways to plot what I want.
library(dplyr)
library(tidyr)

set.seed(1234)

df$origin <- "simulation"
df2$origin <- "real"

df3 <- df2[ , names(df2) %in% names(df)]
df_all <- rbind(df, df3) 

#str(df_all)
#names(df_all)[3]
#tail(names(df_all), 1)
df_all_melted <- gather(df_all, factor_key = TRUE, key = "machine", value = "temperature", QRAD.0400.91a3.6f05.000000000000:QRAD.adf0.074e.5988.000000000000)
#str(df_all_melted)

#sample(levels(df_all_melted$machine), 10)
#df_plot <- subset(df_all_melted, machine %in% sample(levels(df_all_melted$machine), 300))

# aprender mutate & summarize
df_means <- df_plot %>%
    group_by(machine, origin) %>%
    summarize(temperature_mean = mean(temperature)) %>%
    ungroup()

#str(df_plot)
#str(df_means)

#str(df$timestamp)
#str(df2$timestamp)

# procurar como colocar o alpha da legenda = 1
ggplot(df_means, aes(y = temperature_mean, x = machine, color = origin)) +
    #geom_jitter(width = 0.2, alpha = 0.1) +
    geom_point(alpha = 1.0) +
    theme_bw() +
    theme(
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
*** gather 
 Gather takes multiple columns and collapses into key-value pairs, duplicating all other columns as needed. You use gather() when you notice that you have columns that are not variables.
 https://www.rdocumentation.org/packages/tidyr/versions/0.8.3/topics/gather
*** sample
sample takes a sample of the specified size from the elements of x using either with or without replacement.
 https://www.rdocumentation.org/packages/base/versions/3.5.3/topics/sample
*** jitter 
 The jitter geom is a convenient shortcut for geom_point(position = "jitter"). It adds a small amount of random variation to the location of each point, and is a useful way of handling overplotting caused by discreteness in smaller datasets.
 https://ggplot2.tidyverse.org/reference/geom_jitter.html
*** means and error bars
http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)
*** others
https://stackoverflow.com/questions/16251966/controlling-the-alpha-level-in-a-ggplot2-legend
http://www.sthda.com/english/wiki/ggplot2-axis-ticks-a-guide-to-customize-tick-marks-and-labels
https://felixfan.github.io/ggplot2-remove-grid-background-margin/
https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf
rcolorbrewer scales
*** TODO to check mutate, summarize, ggrepel
*** TODO to search how to put alpha of subtitles = 1
** monday, 25-03-2019
*** DONE To finish the graphs.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-25 seg 11:26]
:END:      
**** CANCELLED To check if is possible to tag the used QRads by the csv.
:LOGBOOK:  
- State "CANCELLED"  from "REPORT"     [2019-03-25 seg 11:26]
- It is not necessary. The problem is the method to target temperature on the scheduler.
:END:      
**** To get the color :  https://www.datanovia.com/en/blog/top-r-color-palettes-to-know-for-great-data-visualization/
*** DONE To check the R + Python
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-28 qui 13:17]
:END:      
**** DONE To plot in the same jupyter notebook file: Gantt charts and temperature results.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-28 qui 13:17]
:END:      
*** DONE To check and fix the JobCompletio TODO in the NodeSched.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-02 ter 14:42]
:END:      
**** TODO On checkSimulationFinished() -> Its receiving from Batsim : NoMoreStatic and NoMoreExternal.
***** It makes sense, because when Batsim dispatch the last jobs could there are noMoreStatic or External. But, should we killorRejectAllJobs???
**** TODO On jobCompletion() -> If a job is COMPLETED_KILLED the task is killed.
***** We need to resubmit it. Maybe, we should create a new QTask , then kill the current one.
**** TODO To discuss with Clément the policies to reject a task, a job and to killOrRejectJobs()
*** DONE To export to jupyter notebook my script to plot the comparison between mine and Clément's implementation of the NodeSched.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-25 seg 18:33]
:END:
*** DONE To send the links of notebook files (temperature_analyzes and job_allocation_analyzes) to Pierre.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-25 seg 18:35]
:END:      
** tuesday, 26-03-2019
*** DONE To work on the scheduler
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-02 ter 14:43]
:END:      
**** DONE ReSchedule a job killed because it has lower priority than a new one.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-02 ter 14:43]
:END:      
**** The point is, now, it is killing the jobs regarding the simulation time, not the priority of the jobs.
**** BATSIM should recreate the job, neither PyBatsim !!!!
***** So, I need to do something to PyBatsim says to BatSim to recreate the Job.
***** I'm using bs.resubmit(job) and it looks working. When I filter the Batsim log by "resubmit" Im receiving 604 matching, the number of killed jobs.
***** I think that the real problem are in how the simulation are been finished.
***** TODO To check it.
*** I added a new plot for the temperature analyzes.
** wednesday, 27-03-2019
*** I used rpy2 to do a new Jupyter Notebook running Python and R code together. It already has the Job_Allocation and Temperature analyzes.
*** Working on simple workload with less resources, we are receiving rejected jobs.
**** Platform
{
    "qboxes": [
        {
            "wan_lat": "10ms",
            "lan_lat": "1ms",
            "qrads": [
                {
                    "qmobos": [
                        "MOBO-d5b0-27cc-5462-708bcdabae9f"
                    ],
                    "air_conduct_coeff": 10,
                    "cpu_type": "Intel(R) Core(TM) i7-4790K CPU @ 4.00GHz",
                    "bw": "1Gbps",
                    "host_conduct_coeff": 1.6,
                    "lat": "100us",
                    "nb_mobos": 1,
                    "id": "QRAD-d5b0-27cc-5462-000000000000"
                }
            ],
            "lan_bw": "1Gbps",
            "name": "bnp-wai",
            "disk_size": "6e12",
            "wan_bw": "100Mbps",
            "id": "QBOX-bbbb-0000-0000-000000000001",
            "site": "paris"
        }
    ]
}
**** Events
{"timestamp": 0, "type": "qrad_set_target_temperature", "new_temperature": 22, "qrad": "QRAD-d5b0-27cc-5462-000000000000"}
{"timestamp": 0, "type": "site_set_outside_temperature", "new_temperature": 10, "site": "paris"}
{"timestamp": 0, "type": "site_set_outside_temperature", "new_temperature": 12, "site": "bordeaux"}
{"timestamp": 200, "type": "site_set_outside_temperature", "new_temperature": 15, "site": "bordeaux"}
{"timestamp": 500, "type": "machine_unavailable", "resources": ["MOBO-d5b0-27cc-5462-708bcdabae96", "MOBO-d5b0-27cc-5462-708bcdabacb6", "MOBO-d5b0-27cc-5462-708bcdabae9f"]}
{"timestamp": 1000, "type": "site_set_outside_temperature", "new_temperature": 15, "site": "bordeaux"}
**** Workload
{
    "nb_res": 1,
    "profiles": {
        "QJOB-first_0_profile": {
            "com": 0,
            "cpu": 437072.265625,
            "datasets": [
                "QJOB-first:user-input:540624",
                "QJOB-first:docker:162852561",
                "QJOB-first:user-input:41428146"
            ],
            "user": "unknown-user",
            "type": "parallel_homogeneous",
            "priority": 30
        },
        "QJOB-first_1_profile": {
            "com": 0,
            "cpu": 663829.78515625,
            "datasets": [
                "QJOB-first:user-input:540624",
                "QJOB-first:docker:162852561",
                "QJOB-first:user-input:41428146"
            ],
            "user": "unknown-user",
            "type": "parallel_homogeneous",
            "priority": 30
        },

        "QJOB-second_0_profile": {
            "com": 0,
            "cpu": 265625.0,
            "datasets": [
                "QJOB-second:user-input:41428146",
                "QJOB-second:docker:67221727"
            ],
            "user": "unknown-user",
            "type": "parallel_homogeneous",
            "priority": -5
        },

        "QJOB-triplet_0_profile": {
            "com": 0,
            "cpu": 124683.0,
            "datasets": [
                "QJOB-triplet:user-input:0"
            ],
            "user": "unknown-user",
            "type": "parallel_homogeneous",
            "priority": 10
        },
        "QJOB-triplet_1_profile": {
            "com": 0,
            "cpu": 125583.0,
            "datasets": [
                "QJOB-triplet:user-input:0"
            ],
            "user": "unknown-user",
            "type": "parallel_homogeneous",
            "priority": 10
        },
        "QJOB-triplet_2_profile": {
            "com": 0,
            "cpu": 126653.0,
            "datasets": [
                "QJOB-triplet:user-input:0"
            ],
            "user": "unknown-user",
            "type": "parallel_homogeneous",
            "priority": 10
        },

        "QJOB-long_115_profile": {
            "com": 0,
            "cpu": 709986.572265625,
            "datasets": [
                "QJOB-long:docker:1305968769",
                "QJOB-long:user-input:41428146",
                "QJOB-long:user-input:0"
            ],
            "user": "unknown-user",
            "type": "parallel_homogeneous",
            "priority": -5
        },
        
        "QJOB-null-dataset_1_profile": {
            "com": 0,
            "cpu": 421967.13256835897,
            "datasets": null,
            "user": "unknown-user",
            "type": "parallel_homogeneous",
            "priority": -5
        }
    },
    "jobs": [
        {
            "subtime": 420,
            "res": 1,
            "profile": "QJOB-first_0_profile",
            "id": "QJOB-first_0"
        },
        {
            "subtime": 410,
            "res": 1,
            "profile": "QJOB-first_1_profile",
            "id": "QJOB-first_1"
        },
        {
            "subtime": 0,
            "res": 1,
            "profile": "QJOB-second_0_profile",
            "id": "QJOB-second_0"
        },
        {
            "subtime": 0,
            "res": 1,
            "profile": "QJOB-triplet_0_profile",
            "id": "QJOB-triplet_0"
        },
        {
            "subtime": 0,
            "res": 1,
            "profile": "QJOB-triplet_1_profile",
            "id": "QJOB-triplet_1"
        },
        {
            "subtime": 0,
            "res": 1,
            "profile": "QJOB-triplet_2_profile",
            "id": "QJOB-triplet_2"
        },
        {
            "subtime": 0,
            "res": 1,
            "profile": "QJOB-long_115_profile",
            "id": "QJOB-long_115"
        },   
        {
            "subtime": 0,
            "res": 1,
            "profile": "QJOB-null-dataset_1_profile",
            "id": "QJOB-null-dataset_1"
        }
    ]
}
**** Datasets
{"id": "QJOB-first:docker:162852561", "size": 162852561}
{"id": "QJOB-first:user-input:540624", "size": 43523021}
{"id": "QJOB-first:user-input:41428146", "size": 16336}
{"id": "QJOB-second:docker:67221727", "size": 162852561}
{"id": "QJOB-second:user-input:41428146", "size": 16336}
{"id": "QJOB-triplet:user-input:0", "size": 16336}
{"id": "QJOB-long:user-input:41428146", "size": 41428146}
{"id": "QJOB-long:docker:1305968769", "size": 262144}
{"id": "QJOB-long:user-input:0", "size": 32375882}
** REPORT I have continuing waiting for the StorageController implementatio. :WeekReview:
** REPORT I am debugging the last version of Clément and testing differente workloads. :WeekReview:
** REPORT I have started to work on the re-submition            :WeekReview:
** REPORT I fininshed the first plots for the temperature analyzes (1. looking for the means of all rads; 2. looking for whole data for some specific QRads) :WeekReview:
* Week 28-03 / 03-04
** thuersday, 28-03-2019
*** I was think about the energy consumption.
*** I corrected the analyzes on jupyter notebook using the Magic Commands. aka. %%command. To use rpy2 at the correct way.
*** Im reading the initial papers of the Qarnot description and the scheduler problem to start to think about my Objectives and Introduction for my final report.
*** I attended a presentation.
*** REPORT To Check: Is the availableMobos method working properly?? Because we are killing jobs but it shows available mobos !!! :WeekReview:
:LOGBOOK:  
- State "REPORT"     from "DONE"       [2019-03-29 sex 11:01]
- State "DONE"       from "TODO"       [2019-03-29 sex 11:01]
Yes, it was not working correctly. Clément fixed it.
:END:      
***** Line 337 - BoxSched: # Some instances were dispatched by cannot be started yet, return them to the QNode
** friday, 29-03-2019
*** DONE To check the train to Paris and send to samantha.sanchez@imag.fr
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-03-29 sex 11:01]
Asked for the same as Clément.
:END:
*** REPORT Algorithm: Available Resources
 - The method returns the number of available resouces (empty mobos or possible to be preempted).
 - For the first moment, all Mobos count on bkgd, because it depends on the temperature.
 - So, for example: 
   If we have only one Mobo in the QRad:
   At the first moment, it will return: bkgd/low/high : 1/0/0
   If we dispatch for this QRad a LOW priority job and ask the available resources, it should returns: 0/0/1
      The Mobo is running the job, but it is possible to be preempted by a HIGH priority JOB.
   If we dispatch for this QRad a HIGH priority job and ask the available resources, it should returns: 0/0/0
      Because a HIGH priority job can not be preempted.
 - Another example: QRad with 2 Mobos -> 2/0/0
   - Dispatch a LOW priority => 1/0/1
   - Dispatch a HIGH priority => 0/1/0
   - Dispatch a HIGH priority => 0/0/0
*** DONE Re-submission
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-02 ter 14:46]
:END:      
**** Without resubmition: 
simple: Job submitted: 8 , scheduled: 8 , rejected: 0 , killed: 2 , changed: 0 , timeout: 0 , success 6 , complete: 8
1 day: Job submitted: 221 , scheduled: 221 , rejected: 0 , killed: 0 , changed: 0 , timeout: 0 , success 221 , complete: 221
1 week: Job submitted: 11831 , scheduled: 11831 , rejected: 0 , killed: 501 , changed: 0 , timeout: 0 , success 11330 , complete: 11831
2 weeks : Job submitted: 15922 , scheduled: 15922 , rejected: 0 , killed: 588 , changed: 0 , timeout: 0 , success 15334 , complete: 15922

**** Looking to the flow of the process:
***** At the QBox side:
****** There is a job running in a QBox
***** At the QNode side:
****** The QNode dispatches to the same QBox a job with higher priority.
***** At the QBox side:
****** The QBox will kill the running task => Informs BATSIM.
****** The QBox will start to run the new task => Informs BATSIM.
***** At the BATSIM side:
****** BATSIM will take the killed task as COMPLETED_KILLED => Informs the QNode.
****** BATSIM will take the new task as RUNNIN (?? confirm the "therm")
***** At the QNode side:
****** The QNode will receive on JobCompletion the killes task as COMPLETED_KILEED.
****** TODO Then, we need to resubmit it.
**** TODO Change the event time to finish the simulation, the priority of the jobs, and etc, to force preemption situations.
*** TODO Energy Consumption plots
**** TODO To check the parameters
*** TODO Discuss the final status:
 Job submitted: 8 , scheduled: 2 , rejected: 6 , killed: 1 , changed: 0 , timeout: 0 , success 1 , complete: 2
 Here, 1 job was COMPLETED_SUCCESFULY, 1 killed because was running when BATSIM asked to finish the simulation, 6 rejected because were not scheduled before BATSIM ask to finish the simulation.
 So, my point is, the final status should be : rejected: 6, killed: 1, changed: 0, timout: 0, success 1, complete: 1.
 In my opnion, the "complete" does not make a lot of sense.
** sunday, 31-03-2019
*** Working on Re-submission
 Using a workload to enforce a preemption. It is possible to verify that the job is preempted, the new one starts to run and the previous run is killed.
 When the re-submission is done, the new job is created with '#' to be identified.
 Then all other job are been finished and the resubmitted dont.
 I think that the resubmission method just create the new job, but did not add send to pybatsim to be ran. 
 Two options:
 - To try to change the resubmission method.
 - To try to calls resubmission_job() and another method in batsim to ask the job execution.
 I also think that the resumission does not add the event: "EXECUTE_JOB", so it does not look like a job to be executed. Just some job that already ran or something like this.
 Opntio:
 - To try to modify the resubmission method to chang this value.
** monday, 01-04-2019
*** I filled the files to the Mission in Qarnot
*** Working on re-submission
 I have been done the methos in the NodeSched. It is called in the OnJobCompletion if(job.state == COMPLETED_KILLED).
 Then I'm creating a new job, register_job and copying the mainly values as profile, id, etc, and adding the field "metadata" with job_parent, nb_resubmitted ...
 It have been resubmitted, but, something is occuring when the resubmitted job is completed. Because is not possible to access the job.qtask_id when the job arrive on the jobCompletion.
** tuesday, 02-04-2019
*** Working on re-submission                                   :WeekReview:
 In the QBoxSched, when a subqtask would be created, it was considering a "dataset" list for all instances. But, for the empty one it was getting error. Clément fixed it.
 For the re-submission I created a new method on batsim.py to register a job resubmitted.
 OBS: The update_period defined in the qarnotNodeSched changes the decision, aka, changes the final status. 
      Ex: with the simple_more workload, update_period = [100, 140] => 1 killed job; update_period = [150,300] => 0 killed jobs.
*** Results:
**** period_time = 300
***** 2_week: Job submitted: 15922 , scheduled: 17323 , rejected: 0 , killed: 1424 , changed: 0 , timeout: 0 , success 15899 , complete: 17323
***** 1_week: Job submitted: 11831 , scheduled: 12738 , rejected: 0 , killed: 911 , changed: 0 , timeout: 0 , success 11827 , complete: 12738
***** 1_day: Job submitted: 221 , scheduled: 221 , rejected: 0 , killed: 0 , changed: 0 , timeout: 0 , success 221 , complete: 221
***** simple_more: Job submitted: 8 , scheduled: 8 , rejected: 0 , killed: 0 , changed: 0 , timeout: 0 , success 8 , complete: 8
**** period_time = 140
***** 1_week: Job submitted: 11831 , scheduled: 12784 , rejected: 0 , killed: 960 , changed: 0 , timeout: 0 , success 11824 , complete: 12784
***** 1_day: Job submitted: 221 , scheduled: 221 , rejected: 0 , killed: 0 , changed: 0 , timeout: 0 , success 221 , complete: 221
***** simple_more: Job submitted: 8 , scheduled: 9 , rejected: 0 , killed: 1 , changed: 0 , timeout: 0 , success 8 , complete: 9
*** TODO Add the following values in the final status: resubmitted and real killed (maybe, "preempted").
*** DONE Save the output of the simple_more with preemption and put in the scripts repository, to plot the gantt chart.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-02 ter 15:24]
- But, I did not update the qarnotNodeSchedAndrei. I'm still waiting for the StorageController.
:END:      
*** TODO To solve the problems with the packages in the Jupyter Notebook
 python3 -m ipykernel install --user
** wednesday, 03-04-2019
*** DONE To check the jupyter notebook with Pierre.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-05 sex 10:47]
I need to describe the experiments.
:END:      
*** DONE To do the simulation runs on Jupyter Notebook.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-05 sex 10:46]
We decided that we dont have more time to do it. And we will not run the full simulation during the presentation.
I will finish the Jupyter Notebook file about the analyzes to keep it available for any doubts.
:END:      
*** Meeting
**** Mission in Paris
***** To prepare the agenda
***** To prepare the presentation
***** Objectives
****** To show that the platform are full implemented
****** To show the edge computing things
****** ..?
***** TODO Presentation
****** Workflow diagram
******* Simulation of real platform AND our improvements
******* Real components
****** Analyzes
******* Temperature
******* Job Allocation
** REPORT Resubmission was done.                                :WeekReview:
** REPORT About the the resubmission, I created a new method on batsim.py. :WeekReview:
** REPORT Jupyter Notebook for whole analyzes (job allocation and temperature). :WeekReview:
** REPORT I though about analyze the energy consumption.         :WeekReview:
* Week 04-04 / 10-04
** thuersday, 04-04-2019
*** Journée des doctorants
*** I have been started to write the description of analyzes.
** friday, 05-04-2019
*** DONE To decide the experiments metrics
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-05 sex 17:04]
:END:      
**** TODO To write de full descriptions.
*** DONE To define the final version of the diagram
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-05 sex 17:04]
:END:      
**** TODO To draw the slides, step by step
*** Experiment design board                                          :ATTACH:
:PROPERTIES:
:Attachments: experiments.jpg
:ID:       d57b0d1c-3e5e-4e23-8298-7d787a065b0c
:END:

#+NAME: fig: experiments_board
#+ATTR_ORG: :width 500
file:data/d5/7b0d1c-3e5e-4e23-8298-7d787a065b0c/experiments.jpg
**** From the csv, outputs:
***** a. consumed_energy
***** b. jobs
***** c. machine_states
***** d. ps_change
***** e. schedule
***** f. temperature
***** TODO g. new_status [nb_preempted, cpu_burn, data_staging, nb_killed_at_end]
**** Metrics :
***** 1. from f. : get the temperature over time -> To validate that the standard version is correct and close to the real data.
***** 2. from b. : plot the gantt chart
***** 3. from g. : Compare if there are more or less preempted jobs, data_staging.
***** 4. from e. : Compare if the max_values, as waiting_time, are bigger or smaller. 
***** 5. from e. : Compare if the data_transfer_time is bigger or smaller. 
***** 6. from e. : Compare the consumed_joules, time_computing, time_scheduling. Is the diff. smaller or bigger?
**** Overview:
***** TODO 1,2,4,6. The csv are ready, just script it.
***** WAITING 2. Discuss with the team if is usefull to extract the real job allocation data.
***** WAITING 5, 7. We need to implement it.

** monday, 08-04-2019
*** WAITING Draw the slides for the diagrams
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-04-10 qua 10:10] \\
  Waiting review.
:END:      
*** TODO To finish to wite the experiment descriptions
**** TODO To check how and who will implement the modification to get the .csv as 5. and 7.
*** TODO To run the experiment with both schedulers
 | Workload    | NodeSched | NodeSchedAndrei | Diff | Plots       |
 | simple      |           |                 |      |             |
 | simple_more | DONE      | DONE            | TRUE | VISIBLE     |
 | 1_day       | DONE      | DONE            | TRUE | NOT VISIBLE |
 | 1_week      | DONE      | DONE            | TRUE | VSISBLE     |
 | 2_weeks     |           |                 |      |             |
 |             |           |                 |      |             |

**** I worked on the updates between the branches. I got the last version of the temperature branch and updated my version of the scheduler.
**** I need to check the data movements, because the scheduling is not changing.
** tuesday, 09-04-2019
*** DONE To check the data movements, aka., the job allocation of my scheduler.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-11 qui 09:56]
:END:      
**** Looking as working :D. See example: triplet1 and 2 at subtime:300
*** DONE To change the print of names in the GanttChart with Evalys
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-11 qui 18:32]
- If the name_id is deleted, so the ganttChart is plotted without names. Its not the perfect solution. I should check how to do it properly.
:END:      
In the documentation:  labeler – The strategy to label jobs. By default, the jobID column is used to label jobs. To disable the labeling of jobs, simply return an empty string.
So, I deleted all values from the ID Column.
*** DONE To take a look in the Batsim demo [https://gitlab.inria.fr/batsim/batsim/blob/master/demo/BatsimDemo.ipynb], to plot some similar graphs.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-15 seg 10:24]
:END:      
Good link [http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/81-ggplot2-easy-way-to-mix-multiple-graphs-on-the-same-page/]
Gantt Chart with R [https://adrien-faure.fr/post/ganttcharts/]
*** I did the presentation for the diagram.
** wednesday, 10-04-2019
*** DONE To colletct other weeks
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-11 qui 18:33]
:END:      
*** Progress
| Task              | Status         | What is missing? |
| Plot GanttChart   |                |                  |
| Plot Temperature  | DONE           |                  |
| Plot Many metrics | DONE           |                  |
| Plot 4            |                |                  |
| Plot 5            |                |                  |
| Plot 6            |                |                  |
| Plot 7            |                |                  |
| Text Description  | ALMOST         |                  |
| Full experiments  | ALMOST         |                  |
| Workflow diagram  | DONE           |                  |
|                   |                |                  |

* Week 11-04 / 17-04 
** thuesday, 11-04-2019
*** DONE rbind the _schedule.csv to get the columns as keys and the values as values. Also add a column origin = {standard, andrei}
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-11 qui 10:42]
:END:      
*** WAITING To save as csv the new outputs of pybatsim.
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-04-11 qui 20:43] \\
  Coded to be saved as _schedule_plus.csv in the current directory of pybatsim. Check the correct place.
:END:      
*** DONE Count the number of new downloads done by the scheduler.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-15 seg 10:24]
- As Clément said, this number is the numbe of staging jobs.
:END:      
*** We had a meeting with Alex to discuss wat we will present in the Qarnot meeting.

** friday, 12-04-2019
*** DONE Target in 9 plot
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-15 seg 10:23]
:END:      
*** DONE diff_mean of real and target
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-15 seg 10:23]
:END:      
*** Good practices for plots:
https://measuringu.com/graphing-displaying-data/
http://datalearning.eu/wp-content/uploads/2016/02/Data-Viz-Best-Practices.pdf
https://www.amazon.com/dp/0961392177/?tag=stackoverfl08-20
https://www.amazon.com/o/ASIN/0961392142/ref=nosim/gettgenedone-20
*** Good link for legends in R https://cran.r-project.org/web/packages/lemon/vignettes/legends.html
https://rpkgs.datanovia.com/ggpubr/reference/rremove.html
https://rpkgs.datanovia.com/ggpubr/reference/ggarrange.html
*** List of Graphs
**** Real and standard
***** Means
***** Means_diff
***** 9 rads week
**** Standard and new
***** Means
***** Metrics
***** GanttChart
**** DONE Reduce the standard with real
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-15 seg 10:24]
:END:      
** saturday, 13-04-2019
*** I plotted everything in the metrics graph (all metrics in the same graphic, using facet_wrap) and the diff of means for the real data and standard simulation togheter.
** sunday, 14-04-2019
*** I fixed the labels of all graphics, orginized the notebook and put all graphs in sequence of presentation and analyzes.
*** DONE To fix the diagram slides
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-16 ter 12:24]
:END:      
*** DONE To check the diff for the real one.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-16 ter 12:24]
:END:      
*** DONE To add an arrow to show were we are doing the modifications in the new sched
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-16 ter 12:24]
:END:      
** monday, 15-04-2019
*** I worked on the plots, verifyng if the data about the diff of the means plotted was correct.
*** Danilo's call
**** DONE To say that the measurement of the temperatures was achivied soon.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-16 ter 15:48]
:END:      
**** TODO To run different kinds of workloads
**** STARTED How to validate the the simulator looking to the temperatures,considering external factors.
:LOGBOOK:  
- State "STARTED"    from "TODO"       [2019-04-16 ter 15:48]
- Maybe, isolating a QRad, as the Bordeaux.
:END:      
**** DONE TO show the graphs of the diff, unexpected results
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-16 ter 15:48]
:END:      
**** DEFERRED How to explain more preempted
:LOGBOOK:  
- State "DEFERRED"   from "DONE"       [2019-04-16 ter 15:49]
:END:      
**** DEFERRED Staging jobs is how we are simulating. When we want to process and requires an data set, we create a staging_job to simulate the data tranfer.
:LOGBOOK:  
- State "DEFERRED"   from "TODO"       [2019-04-16 ter 15:49]
:END:      
**** DEFERRED What is the difference between the preempted and the rejected jobs?
:LOGBOOK:  
- State "DEFERRED"   from "TODO"       [2019-04-16 ter 15:49]
:END:      
**** DONE QNode to QBox
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-16 ter 15:49]
:END:      
** tuesday, 16-04-2019
*** Qartnot's meeting
**** TODO To validate the simulation                               :Paper:
***** We should use a simple scenario
***** To start comparing the GanttCharts
***** After, the temperatures
**** To use different external events
**** To design a simple environment to study
**** Before looking for the temperatures, we should look to the allocation.
**** The data extracted is the mean for that our
**** Alex has results about the popularity of the data sets, it could justify or not a location-based scheduler.
**** We want to submmit to the Cluster [https://clustercomp.org/2019/] :Paper:
**** TODO To compare with a real isolate scenario
***** TODO To select wich QRad
***** TODO This QRad could be one in Bordeaux, during the weekend.
**** DONE To fix the diagram. The steps 3-5 are not too accurate. The Node does not ask the update of the resources. It is done time by time. :Paper:
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-18 qui 10:46]
:END:      
**** Paper about energy consumption:  Digitalisation, energy and data demand: The impact of Internet trafficonoverall and peak electricity consumption
**** TODO To do a static scheduler to base the paper               :Paper:
***** It means that we will receive as input informations (real_allocation , real_start_time, real_finish_time, real_avg_speed)
** wednesday, 17-04-2019
*** Qarnot's meeting
**** DONE To discuss with Yoan about the workflow diagram.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-17 qua 13:28]
- Verified and fixed with Yoan.
:END:
**** Working on the static scheduler
***** We will consider that all Qboxes have all data sets.
***** We wont do decision about the jobs, just verify the mobo from where it should be sent.
***** I 
*** BATSIM paper on Cluster 2017 [[https://gitlab.inria.fr/batsim/article-cluster17/blob/master/article/cluster17_energy_tradeoffs.pdf][Energy vs Responsiveness Trade-off in EASY Back Filling]] :Paper:
* Week 18-04 / 24-04
** thuesday, 18-04-2019
*** I worked on the staticScheduler.
*** Rodinia, a benchmark to produce the jobs to be runned in the scheduler [https://rodinia.cs.virginia.edu/doku.php]
** friday, 19-04-2019
*** WAITING The workload modified with the real_* .
*** WAITING The rads that are in Bordeaux to be filtered.
*** I was trying to change a lot of things to adapt the static scheduler in a good way. But now starting again from the standard, Im trying to modify as less as possible.
**** Its runnig, but killing jobs on beginning and are not resubmitting.
**** TODO To write the correct values in the jobs.csv for the "real" logs, to be compared with the jobs.csv as output of the static scheduler.
**** TODO To change the real_allocation from Rads to Mobos.
**** TODO Use the dict_mobos to compare with the real_allocation from some job. Then I will now wich QBox has that mobo.
***** So, the dict should connect the mobos and boxes.
** monday, 22-04-2019
*** Easter's holiday
** tuesday, 23-04-2019
*** Easter's holiday
** wednesday, 24-04-2019
*** Clément got to finish the Static-Scheduler.                   :Clément:
*** I started to think about the real structure of my report.
* Week 25-04 / 01-05
** thursday, 25-04-2019
*** Working on my master report
*** [[https://orgmode.org/manual/LaTeX-Export.html#LaTeX-Export][To export to LateX and process as PDF]]
*** [[https://emacs.stackexchange.com/questions/29748/install-ess-with-use-package][To install ESS, to run code in the org mode
]]
*** I learned how to export it to PDF.
*** TODO To check the errors with bibtex. Maybe copy all files from Pedro and try to export the pdf by it.
** friday, 26-04-2019
*** Working on my master report
*** I downloaded papers about SIMGRID, BARSIM and Qarnot platform.
*** Read:                                                           :Paper:
**** [Simgrid: a Toolkit for the Simulation of application Scheduling]
**** [Versatile, Scalable, and Accurate Simulation of Distributed Applications and Platforms
**** [Scheduling Distributed Applications: the SimGrid Simulation Framework]
**** [Cooling Energy Integration in SimGrid]
**** [Adding Storage Simulation Capacities to the SimGrid Toolkit: Concepts, Models, and API]
**** [Batsim: a Realistic Language-Independent Resources and Jobs Management Systems Simulator]
**** [Predicting the Energy Consumption of MPI Applications at Scale Using a Single Node]
**** [Accuracy Study and Improvement of Network Simulation in the SimGrid Framework]
*** I copied some text from each papar to my report file. Now I need to well structure it and check better how to cite these papers.
** sunday, 28-04-2019
*** I fixed the platform workflow, hiding text and reducing the size of the picture.
*** I also organized the order of the references that I got.
** monday, 29-04-2019
*** Meeting paper                                                   :Paper:
**** Skeleton from the Qarnot presentation
**** Agree about the message ?
**** TODO Experiments?? What to do?
*** WAITING Finish the diagram for the paper
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-04-29 seg 15:18] \\
  I cleaned the workflow diagram (reduced the sizes, did the components near from each other and etc..)
  I also need to ask Denis if we will keep or not the IoT submission.
  I wrote the description of all steps. Also, I added the short description for each step again. All already added in the paper.
:END:
*** DONE A new script: to compare the new workloads in the static scheduler.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-29 seg 19:28]
:END:      
**** DONE Workload: 1_day_26_april
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-29 seg 19:28]
:END:      
**** DONE Workload: 3_day_26_april
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-04-29 seg 19:28]
:END:      
*** TODO To check the competitions:
**** https://www.innovstreet.fr/index.html
**** https://techandrelationships.org/hackathon/
** tuesday, 30-04-2019
*** TODO To verify how to show the job_allocation, now it is just TRUE or FALSE.
*** TODO To check if we will put the workflow diagram in the paper, if, which one? Both or just one about the platform?
*** TODO To check the description and design of the diagram in the paper.
** wednesday, 01-05-2019
*** DONE On paper                                              :WeekReview:
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-01 qua 22:37]
:END:      
**** DONE Platform description based on Yannik's paper.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-01 qua 22:37]
:END:      
**** DONE Scheduler description
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-01 qua 22:37]
:END:      
**** DONE Job Allocation problem
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-01 qua 22:37]
:END:      *
* Week 02-05 / 08-05
** thursday, 02-05-2019
*** DONE To compare the means properly, for the Static Scheduler. The qarnot_extracted_temperatures.csv has the mean measured for each hour. Our output has the timestamp for each 600s.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-03 sex 14:09]
:END:      
*** Key-note: Contrasting artificial intelligence with human intelligence
 Jean-Louis Dessalles - Telecom Paristech
** friday, 03-05-2019
*** I fixed the comparison between the means as the DONE topic one day before.
*** I did the same for the first script
*** I added a filter to see how the machines from Florestine behaves.
** monday, 06-05-2019
*** Qarnot's meeting
**** Paper
***** Schedulers
****** Location based
******* It does change the job allocatoin a lot because there are not a lot of jobs dependents of the same data sets.
***** Experiments
****** Job Allocation
******* Workloads??
******** There are some workloads with more dependecies than others.
******** DONE To check with ALex which one.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-06 seg 19:00]
- They dont know specifically which one is good or not. So, I did a script to describe the workloads.
:END:      
******** TODO To define how many workloads we will simulate
******** There are examples of plot in the Batsim paper showing results for different workloads.
****** Data Movements
******* Staging jobs
******* Preempted jobs
*** Maybe we will change to IEEE MASCOTS
*** Plots [[https://towardsdatascience.com/the-art-of-effective-visualization-of-multi-dimensional-data-6c7202990c57][Good Blog]]
*** Seaborn [[https://seaborn.pydata.org/generated/seaborn.kdeplot.html][plots]]
*** DONE Experiment
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-06 seg 19:00]
:END:      
**** DONE Pie plots
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-06 seg 19:00]
:END:      
**** DONE Denstivie Charts (jobs proc_time)
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-06 seg 19:00]
:END:      
 Using both, the pie plot and the densitive chart regarding the processing time, is possible to justify, somehow, the situations:
 Looking for the Job_Allocation, Pie_plot and Densitive_chart
 if:
 A) The Pie_plot show a strong depency of a big number of data sets
 B) The Densitive_Charts shows that the jobs got a long time to process the job

 A+B) Then, the Job_Allocation should look sparse as well, because although the Pie_plot show a strong depedency of data sets, the densitive_chart show a long time of processing means that the machines will be
   unavailable for a log time, this way, to process the different jobs, it will be allocated to different machines.

 A+B) In the same idea, if the jobs are executed fast and the pie plot show a strong dependency, the job_allocation should show a some machine as cluster of jobs, because, the jobs ends fast and then others
   that depends on the same data sets will be allocated there.
** tuesday, 07-05-2019
*** DONE To extract new workloads to use the script.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-07 ter 16:59]
:END:
*** I added a new plot in the data_sets_dependency analuzes. I'm filtering the jobs by a specific data set, ex: Null.
*** I executed the new script with the new workloads. The problem is that the updated version of workloads are available only from 24/05
*** so, we do not have only 1 week to use. I will try to plot everything with 3 or 5 days.

** wednesday, 08-05-2019
*** WAITING To run the standard simulation using Robin.
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-05-08 qua 22:14] \\
  Some error ??
:END:      
*** DONE To update the location-based using the last standard version
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-08 qua 22:14]
:END:
*** I ran both simulator using 1week_22April and 25April
*** I ran the script to compare the GanttChart, but, the results are the same as the last time, just few differences.
*** The temperature.csv for the location_based is empty. Need to check it.
* Week 09-05 / 15-05 
** thursday, 09-05-2019
*** DONE To update the location based scheduler regarding the last modifications in the standard one.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-09 qui 17:45]
:END:      
*** DONE Extract more workloads
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-09 qui 10:22]
:END:      
**** 1day: 25/04, 29/04, 04/05
**** 3days: 26/04, 03/05
*** DONE Run the simulation
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-09 qui 17:45]
:END:      
*** DONE Plot the log-analyzes (pre-analyzes) and the gantt-chart.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-09 qui 17:45]
:END:      
*** TODO Ask Alex about the data-transfer validation
*** DONE Analyze also the outputs of pybatsim: nb_preempted_jobs, staginf_jobs and etc.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-09 qui 17:45]
:END:      
*** I am thinking about the workloads size. It could be: 5 days for the week and more 2 days for the weekend, separetly.
*** DONE Table of experiments
| Workload   | Standard | Location_based | Log_Analyzes | Allocation_Analyzes |
| 1d_25April | DONE     | DONE           | DONE         | DONE                |
| 1d_29April | DONE     | DONE           | DONE         | DONE                |
| 1d_04May   | DONE     | DONE           | DONE         | DONE                |
| 3d_26April | DONE     | DONE           | DONE         | DONE                |
| 3d_03May   | DONE     | DONE           | DONE         | DONE                |
| 1w_25April |          |                |              |                     |
| 1w_29April | DONE     |                |              |                     |
*** Attention: '1d_04May_two_months' -> All jobs have start_time and finish_time = 0
*** Salah's presentation: One can only gain by replacing EASY Backfilling: A simple scheduling policies case study
*** TODO Analyze all plots together.
** monday, 13-05-2019
*** DEFERRED To order the list of QBoxe by the len(datasets)
:LOGBOOK:  
- State "DEFERRED"   from "DONE"       [2019-05-13 seg 17:24]
- State "DONE"       from "TODO"       [2019-05-13 seg 17:24]
:END:      
*** DONE To fix the DoDispatch to the location based.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-13 seg 17:24]
:END:      
**** I think I'm doing it wrong. I looking the BKGD with dataset + without dataset, then LOW with dataset + without, then HIGH with dataset + without.
**** It should be BKGD with datasets, then LOW with datsets, then HIGH with datasets , THEN, BKGD, LOW and HIGH without datasets.
*** I also created a new notebook to analyze only the allocation.
*** I want to do a script to visualize how much equal were the allocated resources (%).
** tuesday, 14-05-2019
*** I extracted more two workloads: 1w_29April 1w_06May
*** DONE Table of experiments
| Workload   | Standard | Location_based | Log_Analyzes | Allocation_Analyzes |
| 1d_25April | D        | D              | D            |                     |
| 1d_29April | D        | D              | D            |                     |
| 1d_04May   | D        | D              | D            |                     |
| 3d_26April | D        | D              | D            |                     |
| 3d_03May   | D        | D              | D            |                     |
| 1w_29April | DONE     | D              | D            | D                   |
| 1w_06April | DONE     | D              | D            | D                   |
*** I wrote in the paper more two paragraphs about the platform and another two for the experiments explanations.
** wednesday, 15-05-2019
*** Qarnot's meeting
**** TODO Ask:
***** Which workload? How many time? Statistic definitions ...
***** To chekc how will they show the data movements
*** At the beggining of the simulation there are no data sets nowhere, so, the beggining should be the same for both schedulers.
*** TODO Distribution of jobs by priority -> To understande the standart Gantt Chart
*** I think that the gap in the aixe x between the cluster happens because the time_interval = 150,300,....
*** Clemént updated the qarnot-extractor, so I need to exctract all workloads again and rerun everything
*** I fixed the analyze_results.py and also add a method to plot the Gantt Chart diff and now the output are been saved on outputs/[workload]
*** I did a bash script to do the whole analyzes, which are included:
**** Copy the input data from the simulator-prototype
**** Prepare the environment, aka. data/ outputs/
**** Pre process the .csv needed, aka. _jobs.csv
**** Copy the analyze_results.py to all workload folder in data/
**** Execute the analyze_results.py
* Week 16-05 / 22-05
** thursday, 16-05-2019
*** I have been worked on the script run-experiments.sh
**** I also added the robin scripts into that and Moved the script to the main repository *experiments-mascots2019*
**** TODO Modify the output of robin to standard_scheduler and locationBased_scheduler in a folder with "workload_name" as usual.
*** I updated the analyze_results.py and added methods to generate the plot for the data sets dependencies and processing time density.
*** ThomasThomas Ropars' Seminar:
Using Big Data Solutions to Improve HPC systems
   In-situ data analysis
   CPU overheating prediction in HPC systems
*** To conifgure my file in the qarnot-extractor: https://gitlab.inria.fr/anr-greco/influxDB/blob/master/learning/src/utils.py
*** DONE Table of experiments
| Workload   | Extract | Standard | Location_based | Log_Analyzes | Allocation_Analyzes |
| 1d_25April | d       |          |                |              |                     |
| 1d_29April | d       |          |                |              |                     |
| 1d_04May   | d       |          |                |              |                     |
| 3d_26April | d       |          |                |              |                     |
| 3d_03May   | d       |          |                |              |                     |
| 1w_06May   | d       |          |                |              |                     |
| 1w_29April | d       |          |                |              |                     |
*** I extracted the new 'jobs' for all workloads and deleted the no long used ones.
*** TODO To push the sample-data
*** I added both scripts, run-experiments.sh and analyze_results.py to the repo *experiments-mascots2019*
*** I also fixed little bugs with strings in the script create-yaml.py in the repo *experiments-mascots2019*
** friday, 17-05-2019
*** DONE To finsih the run-experiments.sh
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-17 sex 16:34]
:END:      
**** I need to fix the prints. Its printing 3d_ for every steps in the analyzes steps.
*** DONE To run it.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-17 sex 16:35]
:END:      
*** TODO To fix my parts in the paper.
*** DONE To discuss the paper's objective
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-17 sex 16:35]
:END:      
*** TODO To fix the name of the plots to keep near alphabeticaly the GantT_chats.
** saturday, 18-05-2019
*** DONE Added the new schedulers in the script
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-19 dom 14:26]
:END:      
*** DONE Run the script with the new schedulers
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-19 dom 14:26]
:END:      
*** DONE Extract the workloads again, since the qarnot-extractor had changes
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-19 dom 14:26]
:END:
*** I got a template for my report in the MOSIG website and started to edit it.
** sunday, 19-05-2019
*** Analyzes based on the out_pybatsim.csv:
**** I moved the code in R in the jupyter notebook to a R script.
**** I removed the workload dependency to be able to run it using the run-experiments.sh
**** I added the new scheduler to the script, now it compare metrics for 4 schedulers.
*** I ran the script.Erros: 
**** 1w_09 does not finish
**** 1w_02 replicateOnSubmit does not create out_pybatsim.csv, it creates out_pybatsim_full_replicate.csv
***** TODO Ask Clement to fix it.
*** Analyzes:
**** Some histograms are weird, checking:
***** 1d_01May, no data sets -> Workload: datasets: null
***** 1w_25April, no data sets -> Workload: datasets: null
***** 3d_26April, no data sets -> Workload: datasets: null
**** DONE Extract the data sets again
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-19 dom 14:26]
- All the same. 
:END:
**** TODO Ask Yoan why.
*** Since I have sure that the burn and staging jobs are removed from the out_jobs.csv the Gantt Charts are almost empty.
*** Then, to check if now it is correct I did, manually for 1 week out_jobs.csv:
a) Removed all the real jobs and plotted the GanttChart, which means that there are only staging and burn jobs.
b) Removed all the staging and burn and plotted the GanttChart, which means that there are only real jobs
c) For b) I also plotted the chart with details
d) Plotted all together to go from a), b), c) and d) comparing the pictures
*** It really looks like the QRas are heated by burn jobs almost all the time. 
**** The amount of burn jobs in the gantt charts are much much much bigger than the real jobs.
*** I have fixed my text in the paper
*** I extracted more 3 weeks:
**** 26April -> extracted so fast .. Check it.
**** 03May
**** 10May
** monday, 20-05-2019
*** Analyzing
**** 10_May did not work
**** I extracted wrong the 23April, it should be 26April
*** Workin on the graphs
**** Modifications indicated by Clement in the metrics one
**** For the histograms, the aixe x is weird
*** I refigured the graphs:
**** Scheduling metrics
**** Processing time density
***** I made another one filtering by processing_time <= 50000
**** Data sets dependcies
***** I changed the plot from a pie plot to a bar plot
**** Gantt Chart
***** I plotted a new one filtering by starting_time <= 40000
**** As Clement fixed some bug in the events, he ran the 1w_10May, 1d_08May and 3d_03May.
**** I got the outputs and used the updated scripts on them.
** tuesday, 21-05-2019
*** Greco's meeting
**** Paper Mascots
*** Analyzing the last plots:
**** Workloads description
***** 1 week: 10 May
****** Looking for the big picture: 75% in [0,4000]
****** Looking the interval [0,600]: 75% in [0,264]
****** Datasets depencies is not so well distributed.
****** TODO Maybe plot the % and remove the discrete value.
**** New metrics:
***** from data_staging.csv:
****** Compute the SUM of "transfer_size" for "default" status
***** from out_jobs.csv:
****** Compute the Mean and Max for:
******* Waiting_time <- start_time(last resubmitted) - Submission_time(first)
******* turnarooung_time <- completion_time(last resubmitted) - submission_time(first)
*** Christian's phd defense
**** Examples of HPC usage 
**** Problem: A lot of Energy Consummed
**** Solution: Energy consumption model in simulations -> His thesis.
**** Presentation sections organized by Contributions       :MasterThesis:
*** TODO To keep the consumed_joules in the metrics and cite Chrisitian paper
*** TODO Ask how to get the 3 and 10 for qarnotNodeSchedReplicateLeastLoaded.py
*** TODO To validate the waiting and turn around time
*** TODO To change the scale of the data_transfer
*** I added the new metrics: waiting/turn_around_time, total_data_transfer_size
*** I updated the run_measurements.sh to plot the new metrics and also using the new scheduler: Replicate3LeastLoaded Replicate10LeastLoaded
** wednesday, 22-05-2019
*** I finished the plots
*** I fixed the diagrams
*** I finished the scripts and Clement ran it in his computer
**** There were some fixes to be done in about the waiting time, the turn around time was changed for slowdown
*** I wrote the metrics results as a table to be showed in the paper.
* Week 23-05 / 29-05
** thursday, 23-05-2019
*** I have been started to write the section that explain the Qarnot's extractor.
** friday, 24-05-2019
*** I went to Annecy for the EmbrWave Hackathons
** saturday, 25-05-2019
*** I was in Annecy for the EmbrWave
*** I wrote some more lines about the Qarnot's extractor and reviews the text
** sunday, 26-05-2019
*** I reviewd the text
** monday, 27-05-2019
*** We worked in the details of the paper, the last TODO's.
** tuesday, 28-05-2019
*** We discussed by video the last modifications and review for the subsmission.
*** We submitted the paper :D
** wednesday, 29-05-2019
*** I have been started to work on my master thesis, finally.
*** I got the the template that I already modified, added the Jury and copied the paper there.
**** This way I have an idea how many pages does the papar fill.
**** I added some plots for the same reason.
*** TODO Write more in the report
*** TODO Write an workload to validate the schedulers.
* Week 30-05 / 05-06
** thursday, 30-05-2019
*** DONE Organize the master2-project repository with the last outputs to do the new plots
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-31 sex 01:39]
:END:      
**** 1w_03 _10 _17
*** DONE Calclculate the processing_time for the dyn-stag
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-31 sex 02:17]
:END:
*** DONE Plot all workloads in the same metric plots
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-31 sex 01:39]
:END:
*** DONE Plot the processing time as boxplot. Also 1 boxplot for each workload with the same size.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-05-31 sex 01:40]
:END:
*** Does not make sense to plot all data sets dependencies together because the x aixe use the data_set ID and
*** the data sets changes among the workloads.
** friday, 31-05-2019
*** I organized my travel to come back to Brazil (Flight and CROUS)
** sunday, 02-06-2019
*** DONE Write the algorithms
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-02 dom 21:31]
:END:      
**** DONE Standard
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-02 dom 21:31]
:END:      
**** DONE LocalityBased
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-02 dom 21:31]
:END:      
**** DONE FullReplicate
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-02 dom 21:31]
:END:      
**** DONE Replicated3
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-02 dom 21:31]
:END:      
**** DONE Replicated10
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-02 dom 21:31]
:END:      
*** DONE Insert the image of the simulated platform
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-02 dom 14:13]
:END:      
*** DONE Write the explaination of the translation from the real to the simulated platform 
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-04 ter 13:43]
:END:      
*** DONE Write the description of the simulated platform
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-04 ter 13:43]
:END:      
*** TODO Write the 3 topics for the analyzes: processing time, data sets dependencies and metrics. 
**** Maybe, also the allocation
*** DONE To paste the all the plots to figure out how does it fit.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-02 dom 14:03]
:END:
*** TODO Create a simple workload as practical example.
** monday, 03-06-2019
*** WAITING To check with Clement 
:LOGBOOK:  
- State "WAITING"    from "TODO"       [2019-06-03 seg 09:43]
:END:      
**** DONE if the computation of the list of the qbox for each consdition would make difference (for the LocalityBased).
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-06 qui 14:20]
- He said that he did this last modificaton as an "improvement" in the code, to make it cleaner.
:END:      
**** TODO Should I explain details of the StorageController based on the deliverable?
*** DONE To change and run the Locality Based to re-compute the list of available_mobos for each priority.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-06 qui 14:22]
- Nothing changed.
:END:      
*** TODO To merge and run both DoDispatches() on the Locality Based
*** TODO To compare the three results, the previous one, the second one (2 todos above), the third one (1 todo above).
*** I discussed with Denis about the structure. 
**** I should take care about the information of parts not developed by myself.
**** I should explain in the introduction what I used and what I developed. The introduction should have subsections: Edgde, Iot, the platform and my contributions.
*** TODO Some fixes in the Simulated Platform
**** TODO Index starting from 1.
**** TODO Charge the order of 16 and 15.
*** I have been filled the sections of the paper with more details:
**** The Introduction: I pointed what I should add
**** The Qarnot platform: I added the diagram of the workflow and its explanations.
**** The Simulated Platform: I added a new section to show and explain the simulated platform workflow.
**** The Job Allocation: I described a little bit more the scheduling problems and the Qarnot's one and how it is new.
** tuesday, 04-06-2019
*** Sebastian's presentation
**** In Situo Computing fo Data Assimilation
***** Data Assimilation
****** Incorporate Observations into a model (ex Wether Forecast and simulations)
*** I extracted the workload 1w_24May
*** I tried to run it, but there are some errors
*** DONE Ask Clement to run it for some scheduler, or all ..
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-05 qua 18:51]
- I ran it.
:END:      
*** Thesis
**** Introduction: I better structured it, wrote my contributions and emphased what I used and what I developed.
**** Introduction: I also removed some parts that was selling the paper.
*** I changed with Massih my defense to 24 at 14p.m and contact Christophe Cérin that agreed with the time.
*** I invited Yanik to attend to my defense.
*** TODO Try to manage the video conference tool
** wednesday, 05-06-2019
*** I fixed an error in the BoxSched on the rejectReservedMobo.
*** I ran the simulation for the new workload 1w_24May
*** I plotted all the 4 workloads together.
*** TODO Ask Arnaud some opnion of a good way to show it.
*** I almost finish the sectios: 
**** Job Allocation
**** A dedicated Scheduling Simulator for edge platforms
**** Case study: the Qarnot Computing platform
*** I fixed the simulated platform diagram and inserted the new one in the report.
* Week 06-06 / 12-06
** thursday, 06-06-2019 
*** TODO Clement feedbacks
**** TODO We don't say "the section X" but "Section X"
**** TODO I think you should not talk about background tasks, since they are only "cpu burn" not coming from HPC customers 
***** or mention that it is just cpu burn jobs when there are no customer jobs to execute, and I think it's only at QBox level.
**** TODO You should re-check the algorithms, there are some bugs and typos.
**** TODO Re-check also the text, there are some 'not correct' formulations.
**** TODO You tend to forget the subject before a verb. Like "Hence is necessary to", "This way was possible to compare"
*** TODO Advices from Jean-Marques
**** TODO Split the workload in two parts, short and long jobs
**** TODO Show some correlations
*** I did a script to split the workloads, I take the max_time = 75% of the processing time from the workload_original.
*** Now, I have 8 from 4 workloads, each one become _short and _long
*** TODO Run the 8 workloads.
*** I wrote a little bit more about the extractor.
** friday, 07-06-2019
*** TODO If the split workload simulations show difference among the size of jobs
**** then I could say that a mechanism to predict the amount of processing time for the job, in the future, could be very usefuls
*** TODO Added a brief description about the new scheduling policies in the introduction
*** TODO References for scheduling metrics
**** From Salah's paper: Bounded Slowdown , starvation (?)
**** Analyze startvation since it is related with the waiting time
**** threshold.
**** Utilization, predictability, workloads, and user runtime
estimates in scheduling the ibm sp2 with backfilling.
**** Metrics for parallel job scheduling and their convergence.
**** Effect of job size characteristics on job scheduling performance.
***** Maybe I could say about the size since the split of the workloads.
**** Implementing Reproducible Research
*** I added a table with the processing time of all workloads before the split.
*** Using it I justify split the workloads and then, will show the new processing time.
*** Two papers defining Cloud by three kinds (SaaS, PaaS, IaaS)  and 4 "policies" (private, public, community and hybrid)
*** Reading papers for state of the art:

**** Cloud/Edgde
***** Survey on Resource Allocation Policy and Job Scheduling Algorithms of Cloud Computing
 How to make appropriate decisions when
allocating hardware resources to the tasks and dispatching
the computing tasks to resource pool has become the main
issue in cloud computing.
 
Fig.1 shows the mode of calculation in different stages
of development. Calculation mode has gone through a
process from the original mode of gathering all the tasks
to large-scale processor for processing (Fig.1(a)), to the
distributed tasks processing mode based on Internet
(Fig.1(b)) later, and then to the cloud computing mode for
immediate processing (Fig.1(c)) [1] .
 
 Strictly speaking, Cloud Computing
is a model of enabling ubiquitous, convenient and on-
demand network access to a shared pool of configurable
computing resource(e.g. networks, servers, storage,
applications, and service) that can be rapidly provisioned
and released with minimal management effort or service
provider interaction according to NIST(National Institute
of Standards and Technology) [3] .
 
 Cloud computing is a
product from mixing traditional computer techniques and
network technologies, such as grid computing, distributed
computing, parallel computing, utility computing,
network storage, virtualization, load balancing, etc [4] .
 
 Allocation of resources is an important component of
cloud computing. Its efficiency will directly influence the
performance of the whole cloud environment. Since
cloud computing has its own features, original resource
allocation polocy and scheduling algorithms for grid
computing are unable to work under this condition

Background.

Job scheduling of cloud computing refers to the
process of adjusting resources between different resource
users according to certain rules of resource use under a
given cloud environment [21] . Resource management and
job scheduling are the key technologies of cloud
computing. At present, there is not an uniform standard
for job scheduling in cloud. Most alogorithms foucus on ob dispatcher, which is almost resopnsible for all the task
allocations, responses and retransmissions [22] .
***** A Survey on Cloud Computing Resource Allocation Techniques  
ecause of the advancement in Information and
Communication Technology (ICT) over past few years,
Computing has been considered as a utility like water,
electricity, gas and telephony. These utilities are available at
any time to the consumers based on their requirement.
Consumers pay service providers based on their usage [2] [3].

Cloud computing is composed of three kind of services
[1] [5] [6] [9].
1) Cloud Software as a Service (SaaS)
In this service model, instead of using locally run
applications the cloud consumer uses the cloud provider’s
software services running on a cloud infrastructure.
2) Cloud Platform as a Service (PaaS)
In this service model, the cloud platform offers an
environment on which developers create and deploy
applications.
3) Cloud Infrastructure as a Service (IaaS)
In this service model, cloud providers manage large set of
computing resources such as storing and processing
capability.

Could be Private, Public, Community or Hybrid

Motivation
Cloud computing
introduces new challenges for manageable and flexible
resource allocation due to heterogeneity in hardware
capabilities, workload estimation and characteristics in order
to meet Service Level Objectives of the cloud consumers’
applications.
The ultimate goal of resource allocation in cloud
computing is to maximize the profit for cloud providers and
to minimize the cost for cloud consumers.

A list of papers and techniques.
***** A Survey on Mobile Edge Computing: The Communication Perspective

Cloud
 THE LAST decade has seen Cloud Computing emerging
as a new paradigm of computing. Its vision is the central-
ization of computing, storage and network management in the
Clouds, referring to data centers, backbone IP networks and
cellular core networks [1], [2]. The vast resources available in
the Clouds can then be leveraged to deliver elastic computing power and storage to support resource-constrained end-user
devices. Cloud Computing has been driving the rapid growth
of many Internet companies. For example, the Cloud business
has risen to be the most profitable sector for Amazon [3], and
Dropbox’s success depended highly on the Cloud service of
Amazon.

However, in recent years, a new trend in computing is
happening with the function of Clouds being increasingly
moving towards the network edges [4]. It is estimated that
tens of billions of Edge devices will be deployed in the near
future, and their processor speeds are growing exponentially,
following Moore’s Law. Harvesting the vast amount of the
idle computation power and storage space distributed at the
network edges can yield sufficient capacities for perform-
ing computation-intensive and latency-critical tasks at mobile
devices. This paradigm is called Mobile Edge Computing
(MEC) [5]. While long propagation delays remain a key draw-
back for Cloud Computing, MEC, with the proximate access,
is widely agreed to be a key technology for realizing various
visions for next-generation Internet, such as Tactile Internet
(with millisecond-scale reaction time) [6], Internet of Things
(IoT) [7], and Internet of Me [8].

Recently, the concept
of Fog Computing has been proposed by Cisco as a gener-
alized form of MEC where the definition of edge devices
gets broader, ranging from smartphones to set-top boxes [11].
This led to the emergence of a new research area called Fog Computing and Networking [4], [12], [13]. However, the areas
of Fog Computing and MEC are overlapping and the termi-
nologies are frequently used interchangeably.

MEC is implemented based on a virtualized platform
that leverages recent advancements in network functions vir-
tualization (NFV), information-centric networks (ICN) and
software-defined networks (SDN).

A main focus of MEC research is to develop these
general network technologies so that they can be implemented
at the network edges.

**** Grid
***** Survey on Grid Resource Allocation Mechanisms
The need for reliable, pervasive, and high computing
power [32, 63, 64, 93, 94] forces researchers to focus
on low-cost intelligent methodologies for sharing data
and resources such as computers, software applica-
tions, sensors, storage space, and network bandwidth.
As a result, Grid computing was introduced in 1990s
which produced a comprehensive platform for virtual
organizations and computing environments to share
their owned services [136].

A Grid resource can be defined as an entity that
needs to carry out an operation by an application. With
the ever growing size of Grid technology, scheduling
and allocation of Grid resources has become a chal-
lenging and complex research area that has gained
more popularity amongst researchers in recent years.

Grids can be generally classified into homogeneous or
heterogeneous Grids based on factors like operating
system, amount of memory, CPU speed, number of
resources, architecture types and so on [5]. Each
application in Grid environment competes for vari-
ous resources according to application needs. Figure 3
shows the basic classification of Grid resources.

RA mechanisms play an important role in allocating
the most appropriate resources to applications. The
mechanisms perform the allocation of tasks to the
resources in order to ensure QoS to the application
according to the user requirements [48]. Sometimes
RA mechanisms adopt dynamicity whereby resources
are allocated as soon as they are discovered. Such
mechanisms are called dynamic RA mechanisms and
are considered more efficient than the static ones.
Another assumption is that RA mechanisms should be
designed in such a way to avoid underutilization of
resources.
As indicated in Fig. 4, RA mechanisms provide two
basic Grid services:

(a) resource monitoring, and
(b) resource scheduling.

Resource monitoring regularly monitors resource per-
formance, capability, usage and future reservations.
These resources include processors, disks, memories,
and channel bandwidths [48]. The information is then
retrieved by the scheduler that decides on the allo-
cation of the application to the underlying resources.
Some of the widely used Grid resource discovery and
allocation mechanisms based on RMS organizations

are broadly categorized as centralized, distributed, or
hybrid as shown in Fig. 8. Table 3 gives a comparison
of these mechanisms based on their common features.

=> Goals of Table3: Makespan, power minimization, Energy efficiency, task completion time, Data transfer time and others.

==> Table3: techniques and goals.
***** Evaluation of Coordinated Grid Scheduling Strategies

It says that use Slowdown

 As a dynamic performance
metric we use the average bounded slowdown of finished jobs.
The bounded slowdown (BSLD) of a job is defined as:

 It has plots showing 3 workloads and 3 policies for each metric.

**** Job Allocation
***** A survey on resource allocation in high performance distributed computing systems
 The resource management mechanism determines the efficiency of the used resources and guarantees the Quality of
Service (QoS) provided to the users. Therefore, the resource allocation mechanisms are considered a central theme in HPCs
[85].
 QoS resource management and scheduling algorithms are capable of
optimally assigning resources in ideal situation or near-optimally assigning resources in actual situation, taking into account
the task characteristics and QoS requirements [77,86].
 The features of the HPC categories (cluster, grid, and cloud) are conceptually similar [112]. Therefore, an effort has been
made to distinguish each of the categories by selecting relevant distinct features for all. The features are selected based on
the information present in the resource allocation domain, acquired from a plethora of literature.
HPC Jobs
In conservative scheduling [87], processes allocate required resources before execution. Moreover, the
operations are delayed for serial execution of the tasks that helps in process sequencing. The delay is also helpful in rejection
of the processes. In conservative scheduling, operations are not rejected but delayed. In an aggressive (easy) scheduling [87],
operations are immediately scheduled for execution to avoid delay in the operations. Moreover, the operations are reordered
on the arrival of new operations. In some situations when a task cannot be completed in serial way, the operations are re-
jected. In aggressive scheduling, the operations are not delayed but have rejection risk at later stages.


 Clusters
 The goal of cluster computing is to design an efficient computing platform that uses a group of commodity computer re-
sources integrated through hardware, networks, and software to improve the performance and availability of a single com-
puter resource [144,145].
 One of the main ideas of cluster computing is to portray a single system image to the outside
world. Initially, cluster computing and HPC were referred to the same type of computing systems. However, today’s technol-
ogy enables the extension of cluster class by incorporating load balancing, parallel processing, multi-level system manage-
ment, and scalability methodologies.
 The extension of traditional clusters transforms into user-demand systems (provides SLA-based performance) that deliver
Reliability, Availability, and Serviceability (RAS) needed for HPC applications. A modern cluster is made up of a set of com-
modity computers that are usually restricted to a single switch or group of interconnected switches within a single virtual
local-area network (VLAN) [93]. Each compute node (computer) may have different architecture specifications (single pro-
cessor machine, symmetric multiprocessor system, etc.) and access to various types of storage devices. The underlying net-
work is a dedicated network made up of high-speed and low-latency system of switches with a single or multi-level
hierarchic internal structure. In addition to executing compute-intensive applications, cluster systems are also used for rep-
licated storage and backup servers that provide essential fault tolerance and reliability for critical parallel applications.
 
 GRID
 The concept of grid computing is based on using the Internet as a medium for the wide spread availability of powerful
computing resources as low-cost commodity components [142]. Computational grid can be thought of as a distributed sys-
tem of logically coupled local clusters with non-interactive workloads that involve a large number of files [15,111].
 What makes grid different
from conventional HPC systems, such as cluster, is that grids tend to be more loosely coupled, heterogeneous, and geograph-
ically dispersed [139,140].

 Cloud
 Cloud computing describes a new model for Information Technology (IT) services based on the Internet, and typically in-
volves provision of dynamically scalable and often virtualized resources over-the-Internet [2,3,12,147].
 Typical cloud computing providers deliver common business applications online that are ac-
cessed through web service, and the data and software are stored on the servers. Clouds often appear as a single point of
access for computing the consumer needs. Commercial offerings are generally expected to meet the QoS requirements of
customers, and typically include SLAs [9].

Comparisons
 Table 1 depicts the common attributes among the HPC categories, such as size, network type, and coupling.
 
Features and requirements

 Cloud
 The cloud computing systems are difficult to model with resource contention (competing access to shared resources).
Many factors, such as the number of machines, types of application, and overall workload characteristics, can vary widely
and affect the performance of the system.

***** A Comparative Study into Distributed Load Balancing Algorithms for Cloud Computing
This paper considers three potentially viable methods
for load balancing in large scale Cloud systems. Firstly, a
nature-inspired algorithm may be used for self-
organisation, achieving global load balancing via local
server actions. Secondly, self-organisation can be
engineered based on random sampling of the system
domain, giving a balanced load across all system nodes.
Thirdly the system can be restructured to optimise job
assignment at the servers. This paper aims to provide an
evaluation and comparative study of these approaches,
demonstrating distributed algorithms for load balancing.
***** Effect of Job Size Characteristics on Job Scheduling Performance 
certain job size charac-
teristics a ected performance of priority scheduling
signi cantly.

Many previous performance evaluation works as-
sumed that characteristics of parallel jobs, or a paral-
lel workload, followed a simple mathematical model.
However, recent analysis of real workload logs, which
are collected from many large-scale parallel comput-
ers in production use, shows that a real parallel work-
load has more complicated characteristics [5, 6, 7, 8].

Characterization
(a) Uniform model
Job size is an integer that follows the Uniform
distribution within the range [1,m]. This model
is very simple synthetic model that used in many
previous performance evaluation works..
(b) Harmonic model
Job size is an integer that follows the Harmonic
distribution within the range [1,m]. The proba-
bility that jobsize = n is proportional to 1=n 1 : 5 .
This model represents the job size characteristic (1) in the above.
(c) Power2 model
Job size is an integer that is calculated by 2 k
within the range [1,m]. (k is an integer.) The
probability of each value is uniform. This model
represents the job size characteristic (2).
(d) Square model
Job size is an integer that is calculated by k 2
within the range [1,m]. (k is an integer.) The
probability of each value is uniform. This model
represents the job size characteristic (3)
(e) Multi10 model
Job size is an integer that is calculated by 10 1 k
within the range [1,m]. (k is an integer.) The
probability of each value is uniform. This model
represents the job size characteristic (4).

Performance
Performance of job scheduling algorithms is mea-
sured by two metrics: processor utilization and slow-
down ratio. Processor utilization is the percent-
age that processors are busy over entire simulation.
Slowdown ratio, SR, shows normalized data for mean
response time, and it is derived by the following for-
mula [9].
For instance, let us sup-
pose that 10000 jobs were executed in an experiment.
The mean response time of these 10000 jobs was 5
hours, and their mean execution time on processors
was 2 hours. Then, the slowdown ratio is 2.5.

Then, show for each kind of workload the results.

==> Looking my workloads, I could characterize it as its paper presented definitions. 
    Then use the paper results for each workload to power it.
***** An Efficient Algorithm for Scheduling Jobs in Volunteer Computing platforms
 The increased number of processors involved in the new
computing platforms and the complexity of the underly-
ing architecture have made the job scheduling manage-
ment an always more difficult task.
***** Simulation and optimization of HPC job allocation for jointly reducing communication and cooling costs
We implement our evaluation models and allocation optimiza-
tion methods in SST, the Structural Simulation Toolkit. SST is
an architectural simulation framework designed to assist in the
design, evaluation, and optimization of HPC architectures and
applications [33]. It is developed by Sandia National Laboratories
to evaluate the performance of computer systems ranging from
small-scale single-chip processors to large-scale parallel comput-
ing architectures.

This section introduces the simulation framework (SST) we use for
evaluating our joint optimization algorithm managing real-world
parallel workloads, as well as the implementations of job scheduler
and allocation algorithms in SST.

Energy
Along with the data center computational capacities, energy
consumption of HPC data centers has increased tremendously. It
has been reported that the worldwide data center electricity con-
sumption increased by 56% from 2005 to 2010, which accounted
for 1.3% of the total electricity use [8]. A recent review [9] shows
that for every dollar spent on power of data center computing
equipments, another dollar is spent on data center cooling infras-
tructures, which translates to an energy cost reaching up to millions
of dollars and cooling costs reaching close to half of the overall
energy cost [10–12]. Thus, maintaining cooling and energy effi-
ciency is becoming one of the main constraints in the design and
management of today’s data centers.

 ==> Is it possible to say that, then say that Batsim includes the Temperature Plugin from the paper of Cristian.
     Also the Qarnot Platform proposes an efficient solution for that problem.
***** Metrics for Parallel Job Scheduling and Their Convergence
 In some cases interactions may occur between the metric
and certain characteristics of the system, leading to results that actually depend
on the metric being used [21].

 The root cause for convergence problems is variability in the workloads. We
therefore start by characterizing the variability in the runtimes and arrivals of
workloads observed on different systems, and in models based on them.
 
 As we saw above, it may take the average response time a very long time to
converge when the simulated job stream being scheduled is based on dispersive
distributions. Do other metrics converge more quickly? And indeed, what is the
most meaningful metric?
The first metric we deal with is the response time. We define “response time”
to mean the total wallclock time from the instant at which the job is submitted
to the system, until it finishes its run. This can be divided into two components:
the running time T r , during which the job is actually running in parallel on
multiple processing nodes, and the waiting time T w , in which it is waiting to be
scheduled or for some event such as I/O. The waiting time itself can also be used
as a metric, based on the assumption that T r does not depend on the scheduling.
Obviously, a lower bound on the response time of a given job is its running
time. As the runtimes of jobs have a very large variance, so must the response
time. It has therefore been suggested that a better metric may be the slowdown
(also called “expansion factor”), which is the response time normalized by the
running time:
T w + T r
slowdown =
T r
Thus if a job takes twice as long to run due to system load, it suffers from a slow-
down factor of 2, etc. This is expected to reduce the extreme values associated198
D.G. Feitelson
with very long jobs, because even if a week-long job is delayed for a whole year
the slowdown is only a factor of 50. Moreover, slowdown is widely perceived as
better matching user expectations that a job’s response time will be proportional
to its running time. Indeed, 30 years ago Brinch Hansen already suggested that
slowdowns be used to prioritize jobs for scheduling [1].
The problem with the slowdown metric is that it over-em


***** The Emergence of Edge Computing
 oud computing, which has dominated IT
discourse in the past decade, has a twofold
value proposition. First, centralization exploits
economies of scale to lower the marginal
cost of system administration and operations. Second,
organizations can avoid the capital expenditure of
creating a datacenter by consuming computing resources
over the Internet from a large service provider. These
considerations have led to the consolidation of computing
capacity into multiple large datacenters spread across the
globe. The proven economic benefits of cloud computing
make it likely to remain a permanent feature of the future
computing landscape.
However, the forces driving centralization are not the
only ones at work. Nascent technologies and applications
for mobile computing and the Internet of Things (IoT) are
driving computing toward dispersion. Edge computing is anew paradigm in which substantial computing and storage
resources—variously referred to as cloudlets, 1 micro
datacenters, or fog nodes 2 —are placed at the Internet’s
edge in close proximity to mobile devices or sensors.

 Companies
 Industry investment and research interest in edge
computing have grown dramatically in recent years.
Nokia and IBM jointly introduced the Radio Applications
Cloud Server (RACS), an edge computing platform for
4G/LTE networks, in early 2013. 3 The following year, a
mobile edge computing standardization effort began
under the auspices of the European Telecommunications
Standards Institute (ETSI). 4 The Open Edge Computing
initiative (OEC; openedgecomputing.org) was launched in
June 2015 by Vodafone, Intel, and Huawei in partnership
with Carnegie Mellon University (CMU) and expanded
a year later to include Verizon, Deutsche Telekom,
T-Mobile, Nokia, and Crown Castle. This collaboration
 includes creation of a Living Edge
Lab in Pittsburgh, Pennsylvania, to
gain hands-on experience with a live
deployment of proof-of-concept cloudlet-
based applications. Organized by the
telecom industry, the first Mobile Edge
Computing Congress (tmt.knect365.com
/mobile-edge-computing) convened in
London in September 2015 and again
in Munich a year later. The Open Fog
Consortium (www.openfogconsortium
.org) was created by Cisco, Microsoft,
Intel, Dell, and ARM in partnership
with Princeton University in November
2015, and has since expanded to include
many other companies. The First IEEE/
ACM Symposium on Edge Computing
(conferences.computer.org/SEC) was held
in October 2016 in Washington, DC.
***** Edge Computing: Vision and Challenges + The Promise of Edge Computing
 LOUD computing has tremendously changed the way we
live, work, and study since its inception around 2005 [1].
For example, software as a service (SaaS) instances, such as
Google Apps, Twitter, Facebook, and Flickr, have been widely
used in our daily life. Moreover, scalable infrastructures as
well as processing engines developed to support cloud service
are also significantly influencing the way of running business,
for instance, Google File System [2], MapReduce [3], Apache
Hadoop [4], Apache Spark [5], and so on.

====> Data of 2019 !!!!
 
 Now with IoT, we will arrive in the post-cloud
era, where there will be a large quality of data generated
by things that are immersed in our daily life, and a lot of
applications will also be deployed at the edge to consume
these data. By 2019, data produced by people, machines, and
things will reach 500 zettabytes, as estimated by Cisco Global
Cloud Index, however, the global data center IP traffic will
only reach 10.4 zettabytes by that time [9]. By 2019, 45% of
IoT-created data will be stored, processed, analyzed, and acted
upon close to, or at the edge of, the network [10].
 There will be 50 billion things connected to the Internet by 2020, as pre-
dicted by Cisco Internet Business Solutions Group [11]. Some
IoT applications might require very short response time, some
might involve private data, and some might produce a large
quantity of data which could be a heavy load for networks.
Cloud computing is not efficient enough to support these
applications.
Data is increasingly produced at the edge of the network,
therefore, it would be more efficient to also process the data at
the edge of the network. Previous work such as micro datacen-
ter [12], [13], cloudlet [14], and fog computing [15] has been
introduced to the community because cloud computing is not
always efficient for data processing when the data is produced
at the edge of the network.

A. Why Do We Need Edge Computing
1) Push From Cloud Services: Putting all the computing
tasks on the cloud has been proved to be an efficient way
for data processing since the computing power on the cloud
outclasses the capability of the things at the edge. However,
compared to the fast developing data processing speed, the
bandwidth of the network has come to a standstill. With the
growing quantity of data generated at the edge, speed of data
transportation is becoming the bottleneck for the cloud-based
computing paradigm. For example, about 5 Gigabyte data will
be generated by a Boeing 787 every second [16], but the
bandwidth between the airplane and either satellite or base
station on the ground is not large enough for data transmis-
sion. Consider an autonomous vehicle as another example. One
Gigabyte data will be generated by the car every second and it
requires real-time processing for the vehicle to make correct
decisions [17]. If all the data needs to be sent to the cloud
for processing, the response time would be too long. Not to
mention that current network bandwidth and reliability would
be challenged for its capability of supporting a large number
of vehicles in one area. In this case, the data needs to be pro-
cessed at the edge for shorter response time

2) Pull From IoT: Almost all kinds of electrical devices will
become part of IoT, and they will play the role of data pro-
ducers as well as consumers, such as air quality sensors, LED
bars, streetlights and even an Internet-connected microwave
oven. It is safe to infer that the number of things at the edge
of the network will develop to more than billions in a few
years. Thus, raw data produced by them will be enormous,
making conventional cloud computing not efficient enough to
handle all these data. This means most of the data produced
by IoT will never be transmitted to the cloud, instead it will
be consumed at the edge of the network.

 3) Change From Data Consumer to Producer: In the cloud
computing paradigm, the end devices at the edge usually play
as data consumer, for example, watching a YouTube video on
your smart phone. However, people are also producing data
nowadays from their mobile devices. The change from data
consumer to data producer/consumer requires more function
placement at the edge. For example, it is very normal that
people today take photos or do video recording then share
the data through a cloud service such as YouTube, Facebook,
Twitter, or Instagram. Moreover, every single minute, YouTube 
users upload 72 h of new video content; Facebook users share
nearly 2.5 million pieces of content; Twitter users tweet nearly
300 000 times; Instagram users post nearly 220 000 new pho-
tos [18]. However, the image or video clip could be fairly
large and it would occupy a lot of bandwidth for uploading.
In this case, the video clip should be demised and adjusted
to suitable resolution at the edge before uploading to cloud.
Another example would be wearable health devices. Since the
physical data collected by the things at the edge of the net-
work is usually private, processing the data at the edge could
protect user privacy better than uploading raw data to cloud.
 
 C. Edge Computing Benefits
In edge computing we want to put the computing at the
proximity of data sources. This have several benefits com-
pared to traditional cloud-based computing paradigm. Here we
use several early results from the community to demonstrate
the potential benefits. Researchers built a proof-of-concept
platform to run face recognition application in [20], and the
response time is reduced from 900 to 169 ms by moving com-
putation from cloud to the edge. Ha et al. [21] used cloudlets
to offload computing tasks for wearable cognitive assistance,
and the result shows that the improvement of response time is
between 80 and 200ms. Moreover, the energy consumption
could also be reduced by 30%–40% by cloudlet offload-
ing. clonecloud in [22] combine partitioning, migration with
merging, and on-demand instantiation of partitioning between
mobile and the cloud, and their prototype could reduce 20×
running time and energy for tested applications.
***** Containers and Clusters for Edge Cloud Architectures – a Technology Review
Cloud computing is moving from centralised, large-scale
data centres to a more distributed multi-cloud setting com-
prised of a network of larger and smaller virtualised infras-
tructure runtime nodes. Virtualising reaches the network and
allows Internet-of Things (IoT) infrastructures to be integrated.
These architectures and their setting are often referred to as
edge clouds, edge computing or fog computing [4].As a chal-
lenge resulting from distribution, we need a more lightweight
solutions than the current virtual machine (VM)-based virtu-
alisation technology. Furthermore, as another challenge, the
orchestration of lightweight virtualised runtimes is needed
 Regarding the first challenge, the cloud relies on virtual-
isation techniques to achieve elasticity of large-scale shared
resources. Virtual machines (VMs) have been at the core
of the compute infrastructure layer providing virtualised op-
erating systems. We will investigate containers, which are
a lightweight virtualisation concept, i.e., less resource and
time consuming. VMs and containers are both virtualisation
techniques, but solve different problems. Containers are a
solution for more interoperable application packaging in the
cloud and should therefore address the PaaS concerns.

Cloud edge computing is pushing computing applications,
data, and services away from centralized cloud data centre
architectures to the edges of the underlying network [5]. The
objective is to allow analytics and knowledge generation ser-
vices to be placed at the source of the data. Cloud computing
at the edge of the network links into the internet of things
(IoT).
** saturday, 08-06-2019
*** I reviewed all my notes from all read papers. Grouped by main concepts and draft the structure of the text of state of art.
*** I figured out that my workloads split was wrong. I re-built it and then started the simulations again.
** sunday, 09-06-2019
*** I worked more in the state of the art.
*** The simulations finished.
** monday, 10-06-2019
*** First review and re-moldinf of references
**** General idea
This thesis is related with several therms and concepts into the development an egde simulated platform focused 
in the scheduling of HPC jobs. Hence has been studied the state of art of these concepts, it will follow as cluster,
grid and cloud computing concepts, the growth of IoT and Mobile devices as a reasonable usage of this computing 
platforms. As consequence, the edgde computing has been studied and follow some definitions. Regarding the job allocation 
of HPC jobs, will be presented some studies about this kind of jobs and this process from different possible goals
and views.
Running this jobs in that platforms, is necessary to utilize metrics to evaluate the performance for that processes
and techniques. In additional, to handle problems as energy consumption, SOMETHING ELSE, is presented some related
works. Finally, some recent statistics by scientific studies and by viewpoint of company emphasizes the emergency 
and the requirement of egde computing.
**** Cloud/Edgde

Roger's paper
The Internet of Things, Fog and Cloud Continuum: Integration and Challenges


***** Survey on Resource Allocation Policy and Job Scheduling Algorithms of Cloud Computing

Cloud

 
As the [Survey on Resource Allocation Policy and Job Scheduling Algorithms of Cloud Computing], the mode of calculation has been changed along the time, and the Cloud computing defines the current state. It shows this evolution process throwing from 1) the original mode which for processing, gathered all the tasks to large-scale processors, to 2) the distributed tasks processing mode based on Internet, and then to  3) the cloud computing mode for immediate processing.


Job Allocation

 How to make appropriate decisions when
allocating hardware resources to the tasks and dispatching
the computing tasks to resource pool has become the main
issue in cloud computing. 

As the [Survey on Resource Allocation Policy and Job Scheduling Algorithms of Cloud Computing] Cloud computing is a product from mixing traditional computer techniques and network technologies, such as grid computing, distributed computing, parallel computing, utility computing, network storage, virtualization, load balancing, etc [4] .

This way, cloud computing has its own features and the resource allocation policy and scheduling algorithms for the other computing technologies are unable to work under this conditions. For that reason there is not an uniform standard for job scheduling in cloud and then it is an important component in this context. 


***** A Survey on Cloud Computing Resource Allocation Techniques  

Cloud computing is composed of three kind of services [1] [5] [6] [9].
1) Cloud Software as a Service (SaaS), which cloud providers offer software running on a cloud infrastructure.
2) Cloud Platform as a Service (PaaS), which the cloud platform offers an environment for development and deployment of applications.
3) Cloud Infrastructure as a Service (IaaS), which cloud providers manage computing resources such as storing and processing capability.

In additional different deployment models have been adopted based on their variation in physical location, distribution and services, clouds can be classified among a) Private, b) Public, c) Community or d) Hybrid.

Since the cloud computing, new challenges has emerged as the management of flexible resources allocation, due to heterogeneity in hardware capabilities, workload estimation and a variety of services, also as the the maximization of the profit for cloud providers and the minimization of cost for cloud consumers.

A list of papers and techniques.

***** A Survey on Mobile Edge Computing: The Communication Perspective

Cloud

As [A Survey on Mobile Edge Computing: The Communication Perspective] even the last decade has seen Cloud Computing emerging as a new paradigm of computing such that a vision is the centralization of computing, storage and network management in the Clouds, referring to data centers, backbone IP networks and cellular core networks [1], [2]. 
In recent years, it has been changed due to the Clouds being increasingly moving towards the network edges [4]. 


Future

It is estimated that tens of billions of Edge devices will be deployed in the near future, and their processor speeds are growing exponentially, following Moore’s Law. 
Harvesting the vast amount of the idle computation power and storage space distributed at the network edges can yield sufficient capacities for performing computation-intensive and latency-critical tasks at mobile devices. 


MEC

Called Mobile Edge Computing (MEC) [5], the performance of computation of tasks at mobile devices considering the proximate access, is widely agreed to be a key technology for realizing various visions for next-generation Internet, such as Tactile Internet (with millisecond-scale reaction time) [6], Internet of Things (IoT) [7], and Internet of Me [8].

New research area called Fog Computing and Networking [4], [12], [13] are overlapping the terminologies with MEC, in additional, Fog Computing has been proposed by Cisco as a generalized form of MEC where the definition of edge devices gets broader[11]. MEC is implemented based on a virtualized platform such that network functions virtualization (NFV), information-centric networks (ICN) and software-defined networks (SDN).

**** Grid

***** Survey on Grid Resource Allocation Mechanisms and Evaluation of Coordinated Grid Scheduling Strategies (say something generic)

Because the necessity of reliable, pervasive, and high computing power [32, 63, 64, 93, 94] researches has focused on low-cost intelligent methodologies for sharing data and resources such as computers, software applications, sensors, storage space, and network bandwidth. Producing a comprehensive platform for virtual organizations and computing environments the Grid computing was introduced in 1990s [136].

%Get a goof definition of Grid

Grids can be generally classified into homogeneous or heterogeneous Grids based on factors like operating system, amount of memory, CPU speed, number of
resources, architecture types and so on [5]. 

A Grid resource can be defined as an entity that needs to carry out an operation by an application such that each application in Grid environment competes for various resources according to application needs.

This way resource allocation (RA) mechanisms play an important role in allocating the most appropriate resources to applications. The mechanisms perform the allocation of tasks to the resources in order to ensure QoS to the application according to the user requirements [48]. RA mechanisms provide two basic Grid services: (a) resource monitoring, and (b) resource scheduling.

Resource monitoring (a) regularly monitors resource performance, capability, usage and future reservations, including processors, disks, memories,
and channel bandwidths [48]. The information is then retrieved by the scheduler (b) that decides on the allocation of the application to the underlying resources. There are several goals to conduct the RA process as reduce makespan, power minimization and energy efficiency improvement, reduction of task completion time or the amount of data transfer, among others.

**** Job Allocation

***** A survey on resource allocation in high performance distributed computing systems

It presents the three definitios of cluster, grid and cloud computing. Compares three and also define HPC jobs .

The resource management mechanism determines the efficiency of the used resources and guarantees the Quality of
Service (QoS) provided to the users. Therefore, the resource allocation mechanisms are considered a central theme in HPCs
[85]. QoS resource management and scheduling algorithms are capable of optimally assigning resources in ideal situation or near-optimally assigning resources in actual situation, taking into account the task characteristics and QoS requirements [77,86].


The features of the HPC categories (cluster, grid, and cloud) are conceptually similar [112]. Therefore, an effort has been
made to distinguish each of the categories by selecting relevant distinct features for all. 

 Clusters
The goal of cluster computing is to design an efficient computing platform that uses a group of commodity computer resources integrated through hardware, networks, and software to improve the performance and availability of a single computer resource [144,145].

Nowadays, technologies enable the extension of cluster class by incorporating load balancing, parallel processing, multi-level system management, and scalability methodologies.
A modern cluster is made up of a set of commodity computers that are usually restricted to a single switch or group of interconnected switches within a single virtual local-area network (VLAN) [93]. In addition to executing compute-intensive applications, cluster systems are also used for replicated storage and backup servers that provide essential fault tolerance and reliability for critical parallel applications.
 
 GRID
The concept of grid computing is based on using the Internet as a medium for the wide spread availability of powerful computing resources as low-cost commodity components [142]. Computational grid can be thought of as a distributed system of logically coupled local clusters with non-interactive workloads that involve a large number of files [15,111].

What makes grid different from conventional HPC systems, such as cluster, is that grids tend to be more loosely coupled, heterogeneous, and geographically dispersed [139,140].

 Cloud
 
Cloud computing describes a new model for Information Technology (IT) services based on the Internet, and typically involves provision of dynamically scalable and often virtualized resources over-the-Internet [2,3,12,147].
Typical cloud computing providers deliver common business applications online that are accessed through web service, and the data and software are stored on the servers.

The cloud computing systems are difficult to model with resource contention (competing access to shared resources).
Many factors, such as the number of machines, types of application, and overall workload characteristics, can vary widely
and affect the performance of the system.

 Table 1 depicts the common attributes among the HPC categories, such as size, network type, and coupling.




***** A Comparative Study into Distributed Load Balancing Algorithms for Cloud Computing

Techniques as Balancing Algorithm have been applied in this context. As load balancing for distributed algorithms []

This paper considers three potentially viable methods
for load balancing in large scale Cloud systems. 

Firstly, a nature-inspired algorithm may be used for selforganisation, achieving global load balancing via local server actions. 
Secondly, self-organisation can be engineered based on random sampling of the system domain, giving a balanced load across all system nodes.
Thirdly the system can be restructured to optimise job assignment at the servers. 

This paper aims to provide an evaluation and comparative study of these approaches, demonstrating distributed algorithms for load balancing.

***** Effect of Job Size Characteristics on Job Scheduling Performance 

Investigating the effect of the job size on the scheduling performances, the [] has analyzed, characterizes and performed experiments showing how does the bounded slowdown and  processor utilization. In details, processor utilization is the percentage that processors are busy over entire simulation.
Slowdown ratio, SR, shows normalized data for mean response time, and it is derived by the following formula [9].
For instance, let us suppose that 10000 jobs were executed in an experiment. The mean response time of these 10000 jobs was 5 hours, and their mean execution time on processors was 2 hours. Then, the slowdown ratio is 2.5.

Certain job size characteristics affected performance of priority scheduling significantly.

Many previous performance evaluation works assumed that characteristics of parallel jobs, or a parallel workload, followed a simple mathematical model.
However, recent analysis of real workload logs, which are collected from many large-scale parallel computers in production use, shows that a real parallel workload has more complicated characteristics [5, 6, 7, 8].

Characterization

(a) Uniform model
Job size is an integer that follows the Uniform distribution within the range [1,m]. This model is very simple synthetic model that used in many previous performance evaluation works..
(b) Harmonic model 
Job size is an integer that follows the Harmonic distribution within the range [1,m]. The probability that jobsize = n is proportional to 1=n 1 : 5 . This model represents the job size characteristic (1) in the above.
(c) Power2 model
Job size is an integer that is calculated by 2 k within the range [1,m]. (k is an integer.) The probability of each value is uniform. This model represents the job size characteristic (2).
(d) Square model
Job size is an integer that is calculated by k 2 within the range [1,m]. (k is an integer.) The probability of each value is uniform. This model represents the job size characteristic (3)
(e) Multi10 model
Job size is an integer that is calculated by 10 1 k within the range [1,m]. (k is an integer.) The probability of each value is uniform. This model represents the job size characteristic (4).

==> Looking my workloads, I could characterize it as its paper presented definitions. 
    Then use the paper results for each workload to power it.
    
***** An Efficient Algorithm for Scheduling Jobs in Volunteer Computing platforms

 The increased number of processors involved in the new
computing platforms and the complexity of the underly-
ing architecture have made the job scheduling manage-
ment an always more difficult task.

***** Simulation and optimization of HPC job allocation for jointly reducing communication and cooling costs

As BatSim / SimGrid provides a plugin for temperatura due to the necessity of consider the energy consumption in a Grid Platform to execute HPC jobs, there are researches with this specific goal. The [] illustrate it  

We implement our evaluation models and allocation optimization methods in SST, the Structural Simulation Toolkit. SST is an architectural simulation framework designed to assist in the design, evaluation, and optimization of HPC architectures and applications [33]. It is developed by Sandia National Laboratories to evaluate the performance of computer systems ranging from small-scale single-chip processors to large-scale parallel computing architectures.

This section introduces the simulation framework (SST) we use for evaluating our joint optimization algorithm managing real-world parallel workloads, as well as the implementations of job scheduler and allocation algorithms in SST.

Energy

Along with the data center computational capacities, energy consumption of HPC data centers has increased tremendously. It has been reported that the worldwide data center electricity consumption increased by 56\% from 2005 to 2010, which accounted for 1.3\% of the total electricity use [8]. A recent review [9] shows that for every dollar spent on power of data center computing equipments, another dollar is spent on data center cooling infrastructures, which translates to an energy cost reaching up to millions of dollars and cooling costs reaching close to half of the overall energy cost [10–12]. Thus, maintaining cooling and energy efficiency is becoming one of the main constraints in the design and management of today’s data centers.

 ==> Is it possible to say that, then say that Batsim includes the Temperature Plugin from the paper of Cristian.
     Also the Qarnot Platform proposes an efficient solution for that problem.
     
     
***** Metrics for Parallel Job Scheduling and Their Convergence

In some cases interactions may occur between the metric and certain characteristics of the system, leading to results that actually depend on the metric being used [21].

The root cause for convergence problems is variability in the workloads. We therefore start by characterizing the variability in the runtimes and arrivals of workloads observed on different systems, and in models based on them.
 
As we saw above, it may take the average response time a very long time to converge when the simulated job stream being scheduled is based on dispersive distributions. Do other metrics converge more quickly? And indeed, what is the most meaningful metric? 

The first metric we deal with is the response time. We define “response time” to mean the total wall clock time from the instant at which the job is submitted to the system, until it finishes its run. 
This can be divided into two components: the running time Tr , during which the job is actually running in parallel on multiple processing nodes, and the waiting time T w , in which it is waiting to be scheduled or for some event such as I/O. The  waiting time itself can also be used as a metric, based on the assumption that T r does not depend on the scheduling. Obviously, a lower bound on the response time of a given job is its running time. As the runtimes of jobs have a very large variance, so must the response time. It has therefore been suggested that a better metric may be the slowdown (also called “expansion factor”), which is the response time normalized by the running time:
T w + T r slowdown =
T r
Thus if a job takes twice as long to run due to system load, it suffers from a slowdown factor of 2, etc. This is expected to reduce the extreme values associated198 D.G. Feitelson with very long jobs, because even if a week-long job is delayed for a whole year the slowdown is only a factor of 50. Moreover, slowdown is widely perceived as better matching user expectations that a job’s response time will be proportional to its running time. Indeed, 30 years ago Brinch Hansen already suggested that slowdowns be used to prioritize jobs for scheduling [1].
The problem with the slowdown metric is that it over-em


***** The Emergence of Edge Computing

Cloud computing, which has dominated IT
discourse in the past decade, has a twofold
value proposition. First, centralization exploits
economies of scale to lower the marginal
cost of system administration and operations. Second,
organizations can avoid the capital expenditure of
creating a datacenter by consuming computing resources
over the Internet from a large service provider. These
considerations have led to the consolidation of computing
capacity into multiple large datacenters spread across the
globe. The proven economic benefits of cloud computing
make it likely to remain a permanent feature of the future
computing landscape.
However, the forces driving centralization are not the
only ones at work. Nascent technologies and applications
for mobile computing and the Internet of Things (IoT) are
driving computing toward dispersion. Edge computing is anew paradigm in which substantial computing and storage
resources—variously referred to as cloudlets, 1 micro
datacenters, or fog nodes 2 —are placed at the Internet’s
edge in close proximity to mobile devices or sensors.

 Companies
 Industry investment and research interest in edge
computing have grown dramatically in recent years.
Nokia and IBM jointly introduced the Radio Applications
Cloud Server (RACS), an edge computing platform for
4G/LTE networks, in early 2013. 3 The following year, a
mobile edge computing standardization effort began
under the auspices of the European Telecommunications
Standards Institute (ETSI). 4 The Open Edge Computing
initiative (OEC; openedgecomputing.org) was launched in
June 2015 by Vodafone, Intel, and Huawei in partnership
with Carnegie Mellon University (CMU) and expanded
a year later to include Verizon, Deutsche Telekom,
T-Mobile, Nokia, and Crown Castle. This collaboration
 includes creation of a Living Edge
Lab in Pittsburgh, Pennsylvania, to
gain hands-on experience with a live
deployment of proof-of-concept cloudlet-
based applications. Organized by the
telecom industry, the first Mobile Edge
Computing Congress (tmt.knect365.com
/mobile-edge-computing) convened in
London in September 2015 and again
in Munich a year later. The Open Fog
Consortium (www.openfogconsortium
.org) was created by Cisco, Microsoft,
Intel, Dell, and ARM in partnership
with Princeton University in November
2015, and has since expanded to include
many other companies. The First IEEE/
ACM Symposium on Edge Computing
(conferences.computer.org/SEC) was held
in October 2016 in Washington, DC.
***** Edge Computing: Vision and Challenges + The Promise of Edge Computing
 LOUD computing has tremendously changed the way we
live, work, and study since its inception around 2005 [1].
For example, software as a service (SaaS) instances, such as
Google Apps, Twitter, Facebook, and Flickr, have been widely
used in our daily life. Moreover, scalable infrastructures as
well as processing engines developed to support cloud service
are also significantly influencing the way of running business,
for instance, Google File System [2], MapReduce [3], Apache
Hadoop [4], Apache Spark [5], and so on.

====> Data of 2019 !!!!
 
 Now with IoT, we will arrive in the post-cloud
era, where there will be a large quality of data generated
by things that are immersed in our daily life, and a lot of
applications will also be deployed at the edge to consume
these data. By 2019, data produced by people, machines, and
things will reach 500 zettabytes, as estimated by Cisco Global
Cloud Index, however, the global data center IP traffic will
only reach 10.4 zettabytes by that time [9]. By 2019, 45% of
IoT-created data will be stored, processed, analyzed, and acted
upon close to, or at the edge of, the network [10].
 There will be 50 billion things connected to the Internet by 2020, as pre-
dicted by Cisco Internet Business Solutions Group [11]. Some
IoT applications might require very short response time, some
might involve private data, and some might produce a large
quantity of data which could be a heavy load for networks.
Cloud computing is not efficient enough to support these
applications.
Data is increasingly produced at the edge of the network,
therefore, it would be more efficient to also process the data at
the edge of the network. Previous work such as micro datacen-
ter [12], [13], cloudlet [14], and fog computing [15] has been
introduced to the community because cloud computing is not
always efficient for data processing when the data is produced
at the edge of the network.

A. Why Do We Need Edge Computing
1) Push From Cloud Services: Putting all the computing
tasks on the cloud has been proved to be an efficient way
for data processing since the computing power on the cloud
outclasses the capability of the things at the edge. However,
compared to the fast developing data processing speed, the
bandwidth of the network has come to a standstill. With the
growing quantity of data generated at the edge, speed of data
transportation is becoming the bottleneck for the cloud-based
computing paradigm. For example, about 5 Gigabyte data will
be generated by a Boeing 787 every second [16], but the
bandwidth between the airplane and either satellite or base
station on the ground is not large enough for data transmis-
sion. Consider an autonomous vehicle as another example. One
Gigabyte data will be generated by the car every second and it
requires real-time processing for the vehicle to make correct
decisions [17]. If all the data needs to be sent to the cloud
for processing, the response time would be too long. Not to
mention that current network bandwidth and reliability would
be challenged for its capability of supporting a large number
of vehicles in one area. In this case, the data needs to be pro-
cessed at the edge for shorter response time

2) Pull From IoT: Almost all kinds of electrical devices will
become part of IoT, and they will play the role of data pro-
ducers as well as consumers, such as air quality sensors, LED
bars, streetlights and even an Internet-connected microwave
oven. It is safe to infer that the number of things at the edge
of the network will develop to more than billions in a few
years. Thus, raw data produced by them will be enormous,
making conventional cloud computing not efficient enough to
handle all these data. This means most of the data produced
by IoT will never be transmitted to the cloud, instead it will
be consumed at the edge of the network.

 3) Change From Data Consumer to Producer: In the cloud
computing paradigm, the end devices at the edge usually play
as data consumer, for example, watching a YouTube video on
your smart phone. However, people are also producing data
nowadays from their mobile devices. The change from data
consumer to data producer/consumer requires more function
placement at the edge. For example, it is very normal that
people today take photos or do video recording then share
the data through a cloud service such as YouTube, Facebook,
Twitter, or Instagram. Moreover, every single minute, YouTube 
users upload 72 h of new video content; Facebook users share
nearly 2.5 million pieces of content; Twitter users tweet nearly
300 000 times; Instagram users post nearly 220 000 new pho-
tos [18]. However, the image or video clip could be fairly
large and it would occupy a lot of bandwidth for uploading.
In this case, the video clip should be demised and adjusted
to suitable resolution at the edge before uploading to cloud.
Another example would be wearable health devices. Since the
physical data collected by the things at the edge of the net-
work is usually private, processing the data at the edge could
protect user privacy better than uploading raw data to cloud.
 
 C. Edge Computing Benefits
In edge computing we want to put the computing at the
proximity of data sources. This have several benefits com-
pared to traditional cloud-based computing paradigm. Here we
use several early results from the community to demonstrate
the potential benefits. Researchers built a proof-of-concept
platform to run face recognition application in [20], and the
response time is reduced from 900 to 169 ms by moving com-
putation from cloud to the edge. Ha et al. [21] used cloudlets
to offload computing tasks for wearable cognitive assistance,
and the result shows that the improvement of response time is
between 80 and 200ms. Moreover, the energy consumption
could also be reduced by 30%–40% by cloudlet offload-
ing. clonecloud in [22] combine partitioning, migration with
merging, and on-demand instantiation of partitioning between
mobile and the cloud, and their prototype could reduce 20×
running time and energy for tested applications.
***** Containers and Clusters for Edge Cloud Architectures – a Technology Review
Cloud computing is moving from centralised, large-scale
data centres to a more distributed multi-cloud setting com-
prised of a network of larger and smaller virtualised infras-
tructure runtime nodes. Virtualising reaches the network and
allows Internet-of Things (IoT) infrastructures to be integrated.
These architectures and their setting are often referred to as
edge clouds, edge computing or fog computing [4].As a chal-
lenge resulting from distribution, we need a more lightweight
solutions than the current virtual machine (VM)-based virtu-
alisation technology. Furthermore, as another challenge, the
orchestration of lightweight virtualised runtimes is needed
 Regarding the first challenge, the cloud relies on virtual-
isation techniques to achieve elasticity of large-scale shared
resources. Virtual machines (VMs) have been at the core
of the compute infrastructure layer providing virtualised op-
erating systems. We will investigate containers, which are
a lightweight virtualisation concept, i.e., less resource and
time consuming. VMs and containers are both virtualisation
techniques, but solve different problems. Containers are a
solution for more interoperable application packaging in the
cloud and should therefore address the PaaS concerns.

Cloud edge computing is pushing computing applications,
data, and services away from centralized cloud data centre
architectures to the edges of the underlying network [5]. The
objective is to allow analytics and knowledge generation ser-
vices to be placed at the source of the data. Cloud computing
at the edge of the network links into the internet of things
(IoT).
*** I almost finish the state of art, I organized all papers, refered all groups, organized the section and wrote the whole text and structure.
*** DONE Put the bibTex of all references, clean a little bit more and discuss it with Denis.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-11 ter 21:10]
:END:      
*** I plot the results of the last simulation.
*** DONE Analyze the plots.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-11 ter 21:10]
:END:      
*** DONE I figured out that the script to the metrics takes in account the staging_jobs.csv to compute the data transfet and nb_staging_jobs.
:LOGBOOK:  
- State "DONE"       from "TODO"       [2019-06-11 ter 21:10]
I can consider now the output_pybatsim.csv.
:END:      
**** I need to check with Clement which one I should use, since the last updates, the out_pybatsim.csv contains this data. So: Which one is the correct one?
** tuesday, 11-06-2019
*** I finished the State of Art, fixed all the points of Clément, I cleaned all references from the original papers. 
*** I also got all bibTex and correctly referred all papers.
*** I rewokerd the Introduction, added a lit bit more about the Fog, Iot and Edge Computing, referred by the Roger's paper.
*** I added in my contributions: 
**** the design of the experiments, 
**** the measurement of the metrics (bounded_slowdown, slowdown, data_transfer and nb_staging_jobs),
**** the simulation of the Qarnot platform.
*** I analyzed the results with Clément.
*** TODO To write about the results.
*** I have started to change the job_allocation script to get the splited results from the full simulations, but I still think that re-built all was the correct think.
*** TODO To ask Denis what he thinks about it.
** wedenesday
* Weel 13-06 / 19-06
** thurday
** friday, 14-06-2019
*** Final experiments
 | Workloads      | Full experiments | cp ReplicateOnSub FullReplicate | full analyzes | new analyze_results.py | plots | summary(data sets) |
 | 1w_03 - REMOVE | done             | done                            | done          | done                   | done  | done               |
 | 1w_10          | done             | done                            | done          | done                   | done  | done               |
 | 1w_17          | done             | done                            | done          | done                   | done  | done               |
 | 1w_24          | done             | done                            | done          | done                   | done  | done               |
*** I finished the Chapter of experiments.
*** Danilo's meeting
**** We discussed about the bounded slowdown results. Everything made sense, good results :)
** saturday, 15-06-2019
*** I finished the Chapter of Results detailing what I discussed with Danilo.
*** I finished the Conclusion and Abstract.
*** Clement translated my abstrac to French.
** sunday, 16-06-2019
*** I review the whole thesis.
*** I did the spell and grammar check with Grammarly (showed about 900 errors)
*** I fixed the legend in the plots.
*** I organized the order of the plots.
*** Thesis done and delivered.
** monday, 17-06-2019
*** I started to draw the scheduling policies slides, almost done for all policies.
** tuedays, 18-06-2019
*** I got some feedbacks from Clement about the scheduling policies drafts.
*** I almost done the whole presentation, with exception of Conclusion and futher remarks.
*** TODO To decide if I will put the platform workflow.
